# src/cli_enhanced.py
"""Enhanced CLI functions for Transformer support."""

def add_transformer_arguments(parser):
    """Add Transformer-specific arguments to CLI parser."""
    transformer_group = parser.add_argument_group('Transformer Parameters')
    
    transformer_group.add_argument(
        '--d_model',
        type=int,
        default=128,
        help='Transformer model dimension (must be divisible by nhead)'
    )
    
    transformer_group.add_argument(
        '--nhead',
        type=int,
        default=8,
        help='Number of attention heads'
    )
    
    transformer_group.add_argument(
        '--num_layers',
        type=int,
        default=4,
        help='Number of transformer encoder layers'
    )
    
    transformer_group.add_argument(
        '--dim_feedforward',
        type=int,
        default=256,
        help='Dimension of feedforward network'
    )

def enhance_existing_cli_arguments(parser):
    """Enhance existing CLI arguments for better Transformer support."""
    # Learning rate i√ßin Transformer'a uygun range
    parser.add_argument(
        '--learning_rate',
        type=float,
        default=0.001,
        help='Learning rate (automatically adjusted based on model type)'
    )
    
    # Warmup steps for Transformer
    parser.add_argument(
        '--warmup_steps',
        type=int,
        default=1000,
        help='Warmup steps for OneCycleLR scheduler (Transformer only)'
    )

def create_model_config(args):
    """Create complete model configuration from CLI arguments."""
    
    # Base configuration
    config = {
        'model': {
            'type': getattr(args, 'model', 'lstm'),
            'target_mode': args.target_mode,
            'use_focal_loss': args.use_focal_loss,
            'dropout_rate': args.dropout_rate
        },
        'data': {
            'granularity': args.granularity,
            'lookback_candles': args.lookback_candles,
            'sequence_length': getattr(args, 'seq_len', 64)
        },
        'training': {
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'learning_rate': getattr(args, 'learning_rate', 0.001),
            'use_smote': args.oversample_smote
        },
        'api': {
            'api_key': '8d8619f4119fec7e59d73c61b76b480d-d0947fd967a22401c1e48bc1516ad0eb',
            'account_id': '101-004-35700665-002',
            'environment': 'practice'
        }
    }
    
    # Transformer-specific configuration
    if getattr(args, 'model', 'lstm') == 'transformer':
        config['transformer'] = {
            'd_model': getattr(args, 'd_model', 128),
            'nhead': getattr(args, 'nhead', 8),
            'num_layers': getattr(args, 'num_layers', 4),
            'dim_feedforward': getattr(args, 'dim_feedforward', 256),
            'warmup_steps': getattr(args, 'warmup_steps', 1000)
        }
        
        # Transformer i√ßin batch size ayarlamasƒ±
        if not hasattr(args, 'batch_size') or args.batch_size == 32:  # Default deƒüeri
            config['training']['batch_size'] = 16  # Transformer i√ßin daha k√º√ß√ºk
    
    return config

def validate_model_arguments(args):
    """Validate and fix model-specific arguments."""
    
    if getattr(args, 'model', 'lstm') == 'transformer':
        d_model = getattr(args, 'd_model', 128)
        nhead = getattr(args, 'nhead', 8)
        
        # d_model ve nhead uyumluluƒüunu kontrol et
        if d_model % nhead != 0:
            print(f"‚ö†Ô∏è d_model ({d_model}) nhead ({nhead}) ile b√∂l√ºnemiyor!")
            
            # Otomatik d√ºzeltme
            valid_heads = [h for h in [2, 4, 8, 16, 32] if d_model % h == 0 and h <= d_model]
            if valid_heads:
                old_nhead = nhead
                args.nhead = max([h for h in valid_heads if h <= nhead]) or valid_heads[0]
                print(f"üîß nhead otomatik d√ºzeltildi: {old_nhead} ‚Üí {args.nhead}")
            else:
                # d_model'i d√ºzelt
                old_d_model = d_model
                args.d_model = d_model + (nhead - (d_model % nhead))
                print(f"üîß d_model otomatik d√ºzeltildi: {old_d_model} ‚Üí {args.d_model}")
        
        # Transformer i√ßin learning rate ayarlamasƒ±
        if not hasattr(args, 'learning_rate') or args.learning_rate == 0.001:  # Default
            args.learning_rate = 0.0005  # Transformer i√ßin daha d√º≈ü√ºk
            print(f"üîß Transformer i√ßin learning rate ayarlandƒ±: {args.learning_rate}")
    
    return args

def print_model_comparison():
    """Print detailed model architecture comparison."""
    print("""
ü§ñ MODEL ARCHITECTURE COMPARISON

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       Feature       ‚îÇ      LSTM       ‚îÇ    Transformer      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Training Speed      ‚îÇ     ‚ö° Fast     ‚îÇ    üêå Medium        ‚îÇ
‚îÇ Memory Usage        ‚îÇ   üíæ Low (1GB)  ‚îÇ  üíæ High (2-4GB)    ‚îÇ
‚îÇ Long Sequences      ‚îÇ   üìä Limited    ‚îÇ   üìä Excellent      ‚îÇ
‚îÇ Parallelization     ‚îÇ     ‚ùå No       ‚îÇ      ‚úÖ Yes         ‚îÇ
‚îÇ Attention Mechanism ‚îÇ     ‚ùå No       ‚îÇ      ‚úÖ Yes         ‚îÇ
‚îÇ Interpretability    ‚îÇ   üîç Medium     ‚îÇ    üîç High          ‚îÇ
‚îÇ Overfitting Risk    ‚îÇ   ‚ö†Ô∏è Medium     ‚îÇ    ‚ö†Ô∏è High          ‚îÇ
‚îÇ Best For            ‚îÇ Quick training  ‚îÇ Complex patterns    ‚îÇ
‚îÇ GPU Requirements    ‚îÇ   üéÆ Basic      ‚îÇ   üéÆ Modern         ‚îÇ
‚îÇ Batch Size          ‚îÇ   üì¶ 64-128     ‚îÇ   üì¶ 16-32          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üí° RECOMMENDATIONS:
   ‚Ä¢ Use LSTM for: Quick experiments, limited GPU memory
   ‚Ä¢ Use Transformer for: Better accuracy, longer sequences, more data
   
‚öôÔ∏è TRANSFORMER TIPS:
   ‚Ä¢ Start with d_model=128, nhead=8, num_layers=4
   ‚Ä¢ Use smaller batch sizes (16-32)
   ‚Ä¢ Enable gradient clipping (max_norm=1.0)
   ‚Ä¢ Monitor GPU memory usage
""")

def get_model_specific_defaults(model_type: str) -> dict:
    """Get model-specific default parameters."""
    
    if model_type.lower() == 'transformer':
        return {
            'learning_rate': 0.0005,
            'batch_size': 16,
            'epochs': 80,
            'dropout_rate': 0.1,
            'warmup_steps': 1000
        }
    else:  # LSTM
        return {
            'learning_rate': 0.001,
            'batch_size': 32,
            'epochs': 120,
            'dropout_rate': 0.45,
            'warmup_steps': 0
        }

__all__ = [
    'add_transformer_arguments',
    'enhance_existing_cli_arguments', 
    'create_model_config',
    'validate_model_arguments',
    'print_model_comparison',
    'get_model_specific_defaults'
]
