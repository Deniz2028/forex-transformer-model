"""
ƒ∞yile≈ütirilmi≈ü unified training module with best practices.

Bu mod√ºl, LSTM, Transformer ve Enhanced Transformer modelleri i√ßin en iyi uygulamalarla
eƒüitim yeteneklerini saƒülar. DataLoader kullanƒ±mƒ±, uygun scheduler y√∂netimi
ve geli≈ümi≈ü checkpoint sistemi i√ßerir.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data_utils
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Union
from tqdm import tqdm

# Logging yapƒ±landƒ±rmasƒ± - EN BA≈ûTA TANIMLA
logger = logging.getLogger(__name__)

# G√ºvenli import - TransformerClassifier
try:
    from ..models.transformer_model import (
        TransformerClassifier,
        create_data_loaders, 
        set_reproducibility,
        train_transformer_model
    )
    TRANSFORMER_AVAILABLE = True
    logger.info("‚úÖ Transformer model imports ba≈üarƒ±lƒ±")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Transformer model import hatasƒ±: {e}")
    TRANSFORMER_AVAILABLE = False
    TransformerClassifier = None
    create_data_loaders = None
    set_reproducibility = None
    train_transformer_model = None

# Enhanced Transformer import
try:
    from ..models.enhanced_transformer import EnhancedTransformer
    ENHANCED_TRANSFORMER_AVAILABLE = True
    logger.info("‚úÖ Enhanced Transformer model import ba≈üarƒ±lƒ±")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è Enhanced Transformer import hatasƒ±: {e}")
    ENHANCED_TRANSFORMER_AVAILABLE = False
    EnhancedTransformer = None

# Diƒüer importlar
from ..models.lstm import PairSpecificLSTM
from ..models.factory import create_model, get_model_info
from ..models.losses import get_loss_function


# train_enhanced_transformer fonksiyonunu burada tanƒ±mlayalƒ±m
def train_enhanced_transformer(model: 'EnhancedTransformer', X: np.ndarray, y: np.ndarray, 
                              config: Dict[str, Any], device: torch.device) -> Tuple['EnhancedTransformer', Dict]:
    """
    Enhanced Transformer modelini eƒüiten fonksiyon.
    
    Args:
        model: Eƒüitilecek EnhancedTransformer modeli
        X: √ñzellik matrisi [batch_size, seq_len, features]
        y: Hedef deƒüi≈üken
        config: Eƒüitim konfig√ºrasyonu
        device: PyTorch cihazƒ±
        
    Returns:
        (eƒüitilmi≈ü_model, eƒüitim_ge√ßmi≈üi) tuple'ƒ±
    """
    # Eƒüitim parametreleri
    training_config = config.get('training', {})
    epochs = training_config.get('epochs', 30)
    learning_rate = training_config.get('learning_rate', 2e-4)
    batch_size = training_config.get('batch_size', 16)
    patience = training_config.get('patience', 10)
    
    logger.info(f"üöÄ Enhanced Transformer eƒüitimi ba≈ülƒ±yor...")
    logger.info(f"   Epochs: {epochs}, LR: {learning_rate}, Batch: {batch_size}")
    
    # Veriyi tensor'a √ßevir ve DataLoader olu≈ütur
    if create_data_loaders:
        train_loader, val_loader = create_data_loaders(X, y, batch_size)
    else:
        # Fallback: basit veri b√∂lme
        split_idx = int(len(X) * 0.8)
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        X_train = torch.FloatTensor(X_train).to(device)
        y_train = torch.FloatTensor(y_train).to(device)
        X_val = torch.FloatTensor(X_val).to(device) 
        y_val = torch.FloatTensor(y_val).to(device)
        
        train_dataset = data_utils.TensorDataset(X_train, y_train)
        val_dataset = data_utils.TensorDataset(X_val, y_val)
        
        train_loader = data_utils.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = data_utils.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # Optimizer ve loss function
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    
    if model.target_mode == 'binary':
        criterion = nn.BCELoss()
    else:
        criterion = nn.CrossEntropyLoss()
    
    # Scheduler - OneCycleLR (PDF √∂nerisi)
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=learning_rate,
        epochs=epochs,
        steps_per_epoch=len(train_loader),
        pct_start=0.3,
        div_factor=25.0,
        final_div_factor=10000.0
    )
    
    # Eƒüitim ge√ßmi≈üi
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_acc': [],
        'val_acc': []
    }
    
    best_val_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(epochs):
        # Eƒüitim fazƒ±
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            # Forward pass
            optimizer.zero_grad()
            output = model(data)
            
            if model.target_mode == 'binary':
                loss = criterion(output.squeeze(), target)
                pred = (output.squeeze() > 0.5).float()
            else:
                loss = criterion(output, target.long())
                pred = output.argmax(dim=1).float()
            
            # Backward pass
            loss.backward()
            
            # Gradient clipping (PDF √∂nerisi)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step()  # OneCycleLR her step'te g√ºncellenir
            
            # ƒ∞statistikler
            train_loss += loss.item()
            train_correct += pred.eq(target).sum().item()
            train_total += target.size(0)
        
        # Validasyon fazƒ±
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                
                if model.target_mode == 'binary':
                    loss = criterion(output.squeeze(), target)
                    pred = (output.squeeze() > 0.5).float()
                else:
                    loss = criterion(output, target.long())
                    pred = output.argmax(dim=1).float()
                
                val_loss += loss.item()
                val_correct += pred.eq(target).sum().item()
                val_total += target.size(0)
        
        # Ortalamalar
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        train_acc = 100. * train_correct / train_total
        val_acc = 100. * val_correct / val_total
        
        # Ge√ßmi≈üe ekle
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        
        # Early stopping kontrol√º
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        # Log
        if epoch % 5 == 0 or epoch == epochs - 1:
            logger.info(f"Epoch {epoch+1}/{epochs}: "
                       f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, "
                       f"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
        
        # Early stopping
        if patience_counter >= patience:
            logger.info(f"Early stopping at epoch {epoch+1}")
            break
    
    return model, history


class UnifiedTrainer:
    """
    LSTM, Transformer ve Enhanced Transformer modelleri i√ßin birle≈üik trainer.
    
    Her mimari i√ßin uygun optimizer'lar, scheduler'lar ve
    eƒüitim stratejileri ile model-specific optimizasyonlarƒ± i≈üler.
    
    Args:
        device: Eƒüitim i√ßin PyTorch cihazƒ±
        model_type: Model tipi ('lstm', 'transformer' veya 'enhanced_transformer')
        use_focal_loss: Focal loss kullanƒ±lƒ±p kullanƒ±lmayacaƒüƒ±
        target_mode: Tahmin tipi ('binary' veya 'three_class')
        reproducibility_seed: Tekrarlanabilirlik i√ßin seed
    """
    
    def __init__(self, device: torch.device, model_type: str = 'lstm',
                 use_focal_loss: bool = True, target_mode: str = 'binary',
                 reproducibility_seed: int = 42):
        self.device = device
        self.model_type = model_type.lower()
        self.use_focal_loss = use_focal_loss
        self.target_mode = target_mode
        self.pair_models = {}
        self.pair_histories = {}
        
        # Tekrarlanabilirlik ayarla
        if set_reproducibility:
            set_reproducibility(reproducibility_seed)
        
        logger.info(f"üîß UnifiedTrainer ba≈ülatƒ±ldƒ±:")
        logger.info(f"   Model type: {self.model_type.upper()}")
        logger.info(f"   Target mode: {self.target_mode}")
        logger.info(f"   Device: {device}")
        logger.info(f"   Reproducibility seed: {reproducibility_seed}")
        
        # Model kullanƒ±labilirlik kontrol√º
        if self.model_type == 'transformer' and not TRANSFORMER_AVAILABLE:
            logger.warning("‚ö†Ô∏è Transformer modeli kullanƒ±lamƒ±yor, LSTM'e ge√ßiliyor")
            self.model_type = 'lstm'
        elif self.model_type == 'enhanced_transformer' and not ENHANCED_TRANSFORMER_AVAILABLE:
            logger.warning("‚ö†Ô∏è Enhanced Transformer modeli kullanƒ±lamƒ±yor, LSTM'e ge√ßiliyor")
            self.model_type = 'lstm'
        
    def train_pair_model(self, pair_name: str, X: np.ndarray, y: np.ndarray, 
                        config: Dict[str, Any], epochs: int = 120, 
                        batch_size: int = 32) -> Tuple[Union[PairSpecificLSTM, TransformerClassifier, EnhancedTransformer], Dict]:
        """
        Belirli bir d√∂viz √ßifti i√ßin model eƒüit.
        
        Args:
            pair_name: D√∂viz √ßifti adƒ± (√∂rn: 'EUR_USD')
            X: √ñzellik matrisi
            y: Hedef deƒüi≈üken
            config: Model ve eƒüitim konfig√ºrasyonu
            epochs: Eƒüitim epoch sayƒ±sƒ±
            batch_size: Batch boyutu
            
        Returns:
            (eƒüitilmi≈ü_model, eƒüitim_ge√ßmi≈üi) tuple'ƒ±
        """
        logger.info(f"üèãÔ∏è {pair_name} i√ßin {self.model_type.upper()} modeli eƒüitiliyor...")
        
        try:
            if self.model_type == 'lstm':
                return self._train_lstm_model(pair_name, X, y, config, epochs, batch_size)
            elif self.model_type == 'transformer' and TRANSFORMER_AVAILABLE:
                return self._train_transformer_model(pair_name, X, y, config, epochs, batch_size)
            elif self.model_type == 'enhanced_transformer' and ENHANCED_TRANSFORMER_AVAILABLE:
                return self._train_enhanced_transformer_model(pair_name, X, y, config, epochs, batch_size)
            else:
                logger.warning(f"‚ö†Ô∏è Model tipi '{self.model_type}' desteklenmiyor, LSTM kullanƒ±lƒ±yor")
                return self._train_lstm_model(pair_name, X, y, config, epochs, batch_size)
                
        except Exception as e:
            logger.error(f"‚ùå {pair_name} modeli eƒüitim hatasƒ±: {e}")
            raise
    
    def _train_lstm_model(self, pair_name: str, X: np.ndarray, y: np.ndarray, 
                         config: Dict[str, Any], epochs: int, batch_size: int) -> Tuple[PairSpecificLSTM, Dict]:
        """LSTM modeli eƒüitim fonksiyonu."""
        # Factory pattern kullanarak model olu≈ütur
        model = create_model('lstm', config, X.shape[-1], self.device)
        
        # Loss function
        criterion = get_loss_function(
            use_focal_loss=self.use_focal_loss,
            target_mode=self.target_mode,
            device=self.device
        )
        
        # Optimizer
        optimizer = optim.AdamW(model.parameters(), 
                               lr=config.get('training', {}).get('learning_rate', 1e-3),
                               weight_decay=0.01)
        
        # Scheduler
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)
        
        # Basit eƒüitim loop'u (detayƒ±nƒ± sonra geni≈ületebiliriz)
        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
        
        logger.info(f"   ‚úÖ LSTM modeli eƒüitildi (placeholder)")
        return model, history
    
    def _train_transformer_model(self, pair_name: str, X: np.ndarray, y: np.ndarray,
                                config: Dict[str, Any], epochs: int, batch_size: int) -> Tuple[TransformerClassifier, Dict]:
        """Transformer modeli eƒüitim fonksiyonu."""
        if not TRANSFORMER_AVAILABLE or not train_transformer_model:
            raise RuntimeError("Transformer model or training function not available")
        
        # Model olu≈ütur
        model = create_model('transformer', config, X.shape[-1], self.device)
        
        # DataLoader'lar olu≈ütur
        train_loader, val_loader = create_data_loaders(X, y, batch_size)
        
        # Eƒüit
        trained_model, history = train_transformer_model(model, train_loader, val_loader, config, self.device)
        
        return trained_model, history
    
    def _train_enhanced_transformer_model(self, pair_name: str, X: np.ndarray, y: np.ndarray,
                                         config: Dict[str, Any], epochs: int, batch_size: int) -> Tuple[EnhancedTransformer, Dict]:
        """Enhanced Transformer modeli eƒüitim fonksiyonu."""
        if not ENHANCED_TRANSFORMER_AVAILABLE:
            raise RuntimeError("Enhanced Transformer model not available")
        
        # Model olu≈ütur
        model = create_model('enhanced_transformer', config, X.shape[-1], self.device)
        
        # Enhanced transformer eƒüitim fonksiyonunu kullan
        trained_model, history = train_enhanced_transformer(model, X, y, config, self.device)
        
        return trained_model, history
    
    def get_pair_model(self, pair_name: str) -> Union[PairSpecificLSTM, TransformerClassifier, EnhancedTransformer, None]:
        """Belirli bir pair i√ßin eƒüitilmi≈ü modeli getir."""
        return self.pair_models.get(pair_name)
    
    def get_pair_history(self, pair_name: str) -> Dict:
        """Belirli bir pair i√ßin eƒüitim ge√ßmi≈üini getir."""
        return self.pair_histories.get(pair_name, {})
    
    def save_all_models(self, save_dir: str):
        """T√ºm eƒüitilmi≈ü modelleri kaydet."""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        for pair_name, model in self.pair_models.items():
            model_path = os.path.join(save_dir, f"{pair_name}_{self.model_type}.pth")
            torch.save(model.state_dict(), model_path)
            logger.info(f"üíæ {pair_name} modeli kaydedildi: {model_path}")
    
    def load_models(self, load_dir: str, pairs: List[str], config: Dict[str, Any]):
        """Kaydedilmi≈ü modelleri y√ºkle."""
        import os
        
        for pair_name in pairs:
            model_path = os.path.join(load_dir, f"{pair_name}_{self.model_type}.pth")
            
            if os.path.exists(model_path):
                # Model olu≈ütur ve aƒüƒ±rlƒ±klarƒ± y√ºkle
                model = create_model(self.model_type, config, 
                                   config.get('model', {}).get('n_features', 23), 
                                   self.device)
                model.load_state_dict(torch.load(model_path, map_location=self.device))
                model.eval()
                
                self.pair_models[pair_name] = model
                logger.info(f"üìÇ {pair_name} modeli y√ºklendi: {model_path}")
            else:
                logger.warning(f"‚ö†Ô∏è Model dosyasƒ± bulunamadƒ±: {model_path}")
