# src/models/factory.py
"""
GeliÅŸtirilmiÅŸ model factory for creating different model architectures.

Bu modÃ¼l, LSTM, EnhancedTransformer, HybridLSTMTransformer ve diÄŸer modelleri oluÅŸturmak iÃ§in birleÅŸik arayÃ¼z saÄŸlar.
Model seÃ§imi, parametre validasyonu, cihaz yerleÅŸtirme ve geliÅŸmiÅŸ hata yÃ¶netimi
ile kapsamlÄ± konfigÃ¼rasyon doÄŸrulamasÄ± iÃ§erir.
"""

import torch
import copy
import logging
import importlib
from typing import Dict, Any, Union, List, Tuple, Type

# Logger yapÄ±landÄ±rmasÄ± - EN BAÅTA TANIMLA
logger = logging.getLogger(__name__)

# LSTM modeli iÃ§in import (her zaman mevcut)
from .lstm import PairSpecificLSTM, create_model as create_lstm_model

# Transformer modeli iÃ§in koÅŸullu import
TRANSFORMER_AVAILABLE = False
ENHANCED_TRANSFORMER_V2_AVAILABLE = False
HYBRID_MODEL_AVAILABLE = False
EnhancedTransformer = None
TransformerClassifier = None
create_transformer_model = None
create_enhanced_transformer = None
EnhancedTransformerV2 = None
create_enhanced_transformer_v2 = None
HybridLSTMTransformer = None

# Enhanced Transformer iÃ§in import (mevcut dosyadan)
try:
    from .enhanced_transformer import EnhancedTransformer, create_enhanced_transformer
    TRANSFORMER_AVAILABLE = True
    logger.info("âœ… EnhancedTransformer import baÅŸarÄ±lÄ±")
except ImportError as e:
    logger.warning(f"âš ï¸ Enhanced Transformer import hatasÄ±: {e}")
except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata (enhanced_transformer): {e}")

# Transformer modÃ¼lÃ¼nÃ¼ dinamik olarak yÃ¼kleme
try:
    transformer_module = importlib.import_module('.transformer_model', package='src.models')
    
    # TransformerClassifier var mÄ±?
    if hasattr(transformer_module, 'TransformerClassifier'):
        from .transformer_model import TransformerClassifier, create_transformer_model
        logger.info("âœ… TransformerClassifier import baÅŸarÄ±lÄ±")
    else:
        logger.warning("âš ï¸ TransformerClassifier bulunamadÄ±")
        
except ImportError as e:
    logger.warning(f"âš ï¸ Transformer model import hatasÄ±: {e}")
except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata (transformer_model): {e}")

# Hibrit LSTM-Transformer modeli iÃ§in import
try:
    hybrid_module = importlib.import_module('.hybrid_model', package='src.models')
    
    if hasattr(hybrid_module, 'HybridLSTMTransformer'):
        from .hybrid_model import HybridLSTMTransformer, create_hybrid_model, validate_hybrid_config
        HYBRID_MODEL_AVAILABLE = True
        logger.info("âœ… HybridLSTMTransformer import baÅŸarÄ±lÄ±")
        
except ImportError as e:
    logger.warning(f"âš ï¸ Hybrid model import hatasÄ±: {e}")
    HYBRID_MODEL_AVAILABLE = False
except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata (hybrid_model): {e}")
    HYBRID_MODEL_AVAILABLE = False

# Model tipi sabitleri - GÃœVENLÄ° HALE GETÄ°R
SUPPORTED_MODELS = ['lstm', 'pairspecificlstm']

# Mevcut modellere gÃ¶re desteklenen tipleri ekle
if TRANSFORMER_AVAILABLE:
    SUPPORTED_MODELS.extend(['transformer', 'enhanced_transformer'])
    logger.info("âœ… Transformer modelleri desteklenen listeye eklendi")

if HYBRID_MODEL_AVAILABLE:
    SUPPORTED_MODELS.extend(['hybrid_lstm_transformer', 'hybrid'])
    logger.info("âœ… Hibrit LSTM-Transformer desteklenen listeye eklendi")

logger.info(f"ğŸ“‹ Desteklenen modeller: {SUPPORTED_MODELS}")

# Model alias'larÄ±
LSTM_ALIASES = ['lstm', 'pairspecificlstm']
TRANSFORMER_ALIASES = ['transformer', 'enhanced_transformer'] if TRANSFORMER_AVAILABLE else []
HYBRID_ALIASES = ['hybrid_lstm_transformer', 'hybrid'] if HYBRID_MODEL_AVAILABLE else []

# ModelInstance tipini gÃ¼venli hale getir
available_types = [PairSpecificLSTM]

if TRANSFORMER_AVAILABLE:
    if EnhancedTransformer:
        available_types.append(EnhancedTransformer)
    if TransformerClassifier:
        available_types.append(TransformerClassifier)

if HYBRID_MODEL_AVAILABLE and HybridLSTMTransformer:
    available_types.append(HybridLSTMTransformer)

if len(available_types) == 0:
    logger.error("âš ï¸ HiÃ§bir model tipi kullanÄ±lamÄ±yor! Sadece LSTM kullanÄ±lacak.")
    available_types = [PairSpecificLSTM]

ModelInstance = Union[tuple(available_types)]

# Tip alias'larÄ± (daha aÃ§Ä±klayÄ±cÄ± tip notasyonlarÄ± iÃ§in)
ModelConfig = Dict[str, Any]
TransformerConfig = Dict[str, Any]
LSTMConfig = Dict[str, Any]
HybridConfig = Dict[str, Any]


def create_model(
    model_type: str,
    config: ModelConfig, 
    n_features: int, 
    device: torch.device
) -> ModelInstance:
    """
    Tip ve konfigÃ¼rasyona gÃ¶re model oluÅŸturan factory fonksiyonu.
    
    Args:
        model_type: OluÅŸturulacak model tipi ('lstm', 'enhanced_transformer', 'hybrid_lstm_transformer', ...)
        config: Model konfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        n_features: GiriÅŸ Ã¶zellik sayÄ±sÄ±
        device: Model oluÅŸturulacak cihaz
        
    Returns:
        BaÅŸlatÄ±lmÄ±ÅŸ model instance'Ä±
        
    Raises:
        ValueError: Model tipi desteklenmiyorsa veya konfigÃ¼rasyon geÃ§ersizse
        RuntimeError: Model oluÅŸturma sÄ±rasÄ±nda beklenmeyen hata
    """
    model_type_normalized = model_type.lower().strip()
    
    logger.info(f"ğŸ­ Model factory: {model_type_normalized} modeli oluÅŸturuluyor...")
    logger.info(f"   GiriÅŸ Ã¶zellikleri: {n_features}")
    logger.info(f"   Cihaz: {device}")
    
    # Model tipi validasyonu
    if model_type_normalized not in SUPPORTED_MODELS:
        raise ValueError(
            f"Desteklenmeyen model tipi: '{model_type}'. "
            f"Desteklenen modeller: {SUPPORTED_MODELS}"
        )
    
    try:
        if model_type_normalized in LSTM_ALIASES:
            # LSTM konfigÃ¼rasyonu validasyonu
            validated_config = validate_lstm_config(config)
            model = create_lstm_model(validated_config, n_features, device)
            logger.info(f"   âœ… LSTM modeli baÅŸarÄ±yla oluÅŸturuldu")
            
        elif model_type_normalized in TRANSFORMER_ALIASES:
            if not TRANSFORMER_AVAILABLE:
                raise RuntimeError("Transformer modeli desteklenmiyor (import hatasÄ±)")
                
            # Transformer konfigÃ¼rasyonu validasyonu
            validated_config = validate_transformer_config(config)
            
            if model_type_normalized == 'enhanced_transformer' and create_enhanced_transformer:
                model = create_enhanced_transformer(validated_config, n_features, device)
                logger.info(f"   âœ… EnhancedTransformer modeli baÅŸarÄ±yla oluÅŸturuldu")
            elif model_type_normalized == 'transformer' and create_transformer_model:
                model = create_transformer_model(validated_config, n_features, device)
                logger.info(f"   âœ… Transformer modeli baÅŸarÄ±yla oluÅŸturuldu")
            else:
                raise RuntimeError(f"Ä°stenen model tipi desteklenmiyor: {model_type}")
                
        elif model_type_normalized in HYBRID_ALIASES:
            if not HYBRID_MODEL_AVAILABLE or not HybridLSTMTransformer:
                raise RuntimeError("Hibrit LSTM-Transformer modeli desteklenmiyor (import hatasÄ±)")
                
            # Hibrit model konfigÃ¼rasyonu validasyonu
            validated_config = validate_hybrid_config(config)
            model = create_hybrid_model(validated_config, n_features, device)
            logger.info(f"   âœ… Hibrit LSTM-Transformer modeli baÅŸarÄ±yla oluÅŸturuldu")
        
        else:
            # Bu duruma teorik olarak gelmemeli ama gÃ¼venlik iÃ§in
            raise ValueError(f"Model tipi '{model_type}' iÅŸlenemiyor")
        
        # Model bilgilerini logla
        model_info = get_model_info(model)
        logger.info(f"   ğŸ“Š Model parametreleri: {model_info['total_parameters']:,}")
        
        return model
        
    except ValueError as e:
        logger.error(f"   âŒ Model oluÅŸturma hatasÄ±: {e}")
        raise
    except Exception as e:
        logger.error(f"   âŒ Beklenmeyen hata: {e}")
        raise RuntimeError(f"Model oluÅŸturma baÅŸarÄ±sÄ±z: {e}") from e


def validate_transformer_config(config: TransformerConfig) -> TransformerConfig:
    """
    Transformer konfigÃ¼rasyon parametrelerini doÄŸrula ve ayarla.
    
    Args:
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        DoÄŸrulanmÄ±ÅŸ konfigÃ¼rasyon
        
    Raises:
        ValueError: KonfigÃ¼rasyon geÃ§ersizse
    """
    config = copy.deepcopy(config)
    model_config = config.get('model', {})
    
    d_model = model_config.get('d_model', 256)
    nhead = model_config.get('nhead', 8)
    num_layers = model_config.get('num_layers', 4)
    dropout = model_config.get('dropout_rate', 0.1)
    
    logger.info(f"ğŸ” Transformer konfigÃ¼rasyonu doÄŸrulanÄ±yor...")
    logger.info(f"   d_model: {d_model}, nhead: {nhead}, layers: {num_layers}")
    
    # d_model validasyonu
    if not isinstance(d_model, int) or d_model < 64 or d_model > 1024:
        raise ValueError(f"d_model geÃ§ersiz: {d_model}. 64 ile 1024 arasÄ±nda integer olmalÄ±")
    
    # nhead validasyonu
    if not isinstance(nhead, int) or nhead < 1 or nhead > 16:
        raise ValueError(f"nhead geÃ§ersiz: {nhead}. 1 ile 16 arasÄ±nda integer olmalÄ±")
    
    # d_model ve nhead uyumluluÄŸu
    if d_model % nhead != 0:
        valid_heads = [h for h in [1, 2, 4, 8, 16] if d_model % h == 0 and h <= d_model]
        if valid_heads:
            old_nhead = nhead
            nhead = max([h for h in valid_heads if h <= nhead]) or valid_heads[-1]
            model_config['nhead'] = nhead
            logger.warning(f"   ğŸ”§ nhead otomatik dÃ¼zeltildi: {old_nhead} â†’ {nhead}")
        else:
            raise ValueError(f"d_model ({d_model}) nhead ({nhead}) ile bÃ¶lÃ¼nebilir olmalÄ±")
    
    # num_layers validasyonu
    if not isinstance(num_layers, int) or num_layers < 1 or num_layers > 8:
        raise ValueError(f"num_layers geÃ§ersiz: {num_layers}. 1 ile 8 arasÄ±nda integer olmalÄ±")
    
    # dropout validasyonu
    if not isinstance(dropout, (int, float)) or dropout < 0.0 or dropout > 0.5:
        raise ValueError(f"dropout geÃ§ersiz: {dropout}. 0.0 ile 0.5 arasÄ±nda float olmalÄ±")
    
    logger.info(f"   âœ… Transformer konfigÃ¼rasyonu doÄŸrulandÄ±")
    return config


def validate_enhanced_transformer_config(config: TransformerConfig) -> TransformerConfig:
    """
    Enhanced Transformer konfigÃ¼rasyon parametrelerini doÄŸrula ve ayarla.
    
    Args:
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        DoÄŸrulanmÄ±ÅŸ konfigÃ¼rasyon
        
    Raises:
        ValueError: KonfigÃ¼rasyon geÃ§ersizse
    """
    config = copy.deepcopy(config)
    model_config = config.get('model', {})
    
    d_model = model_config.get('d_model', 512)
    nhead = model_config.get('nhead', 8)
    num_layers = model_config.get('num_layers', 6)
    dropout = model_config.get('dropout_rate', 0.1)
    
    logger.info(f"ğŸ” Enhanced Transformer konfigÃ¼rasyonu doÄŸrulanÄ±yor...")
    logger.info(f"   d_model: {d_model}, nhead: {nhead}, layers: {num_layers}")
    
    # Enhanced transformer iÃ§in daha yÃ¼ksek minimum deÄŸerler
    if not isinstance(d_model, int) or d_model < 128 or d_model > 1024:
        raise ValueError(f"Enhanced d_model geÃ§ersiz: {d_model}. 128 ile 1024 arasÄ±nda integer olmalÄ±")
    
    if not isinstance(nhead, int) or nhead < 4 or nhead > 16:
        raise ValueError(f"Enhanced nhead geÃ§ersiz: {nhead}. 4 ile 16 arasÄ±nda integer olmalÄ±")
    
    # d_model ve nhead uyumluluÄŸu
    if d_model % nhead != 0:
        valid_heads = [h for h in [4, 8, 16] if d_model % h == 0 and h <= d_model]
        if valid_heads:
            old_nhead = nhead
            nhead = max([h for h in valid_heads if h <= nhead]) or valid_heads[-1]
            model_config['nhead'] = nhead
            logger.warning(f"   ğŸ”§ Enhanced nhead otomatik dÃ¼zeltildi: {old_nhead} â†’ {nhead}")
        else:
            raise ValueError(f"Enhanced d_model ({d_model}) nhead ({nhead}) ile bÃ¶lÃ¼nebilir olmalÄ±")
    
    if not isinstance(num_layers, int) or num_layers < 2 or num_layers > 12:
        raise ValueError(f"Enhanced num_layers geÃ§ersiz: {num_layers}. 2 ile 12 arasÄ±nda integer olmalÄ±")
    
    if not isinstance(dropout, (int, float)) or dropout < 0.0 or dropout > 0.3:
        raise ValueError(f"Enhanced dropout geÃ§ersiz: {dropout}. 0.0 ile 0.3 arasÄ±nda float olmalÄ±")
    
    logger.info(f"   âœ… Enhanced Transformer konfigÃ¼rasyonu doÄŸrulandÄ±")
    return config


def validate_lstm_config(config: LSTMConfig) -> LSTMConfig:
    """
    LSTM konfigÃ¼rasyon parametrelerini doÄŸrula ve ayarla.
    
    Args:
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        DoÄŸrulanmÄ±ÅŸ konfigÃ¼rasyon
        
    Raises:
        ValueError: KonfigÃ¼rasyon geÃ§ersizse
    """
    config = copy.deepcopy(config)
    model_config = config.get('model', {})
    
    hidden_size = model_config.get('hidden_size', 64)
    num_layers = model_config.get('num_layers', 2)
    dropout = model_config.get('dropout_rate', 0.45)
    
    logger.info(f"ğŸ” LSTM konfigÃ¼rasyonu doÄŸrulanÄ±yor...")
    logger.info(f"   hidden_size: {hidden_size}, layers: {num_layers}, dropout: {dropout}")
    
    # hidden_size validasyonu
    if not isinstance(hidden_size, int) or hidden_size < 16 or hidden_size > 512:
        raise ValueError(f"hidden_size geÃ§ersiz: {hidden_size}. 16 ile 512 arasÄ±nda integer olmalÄ±")
    
    # num_layers validasyonu  
    if not isinstance(num_layers, int) or num_layers < 1 or num_layers > 4:
        raise ValueError(f"num_layers geÃ§ersiz: {num_layers}. 1 ile 4 arasÄ±nda integer olmalÄ±")
    
    # dropout validasyonu
    if not isinstance(dropout, (int, float)) or dropout < 0.0 or dropout > 0.8:
        raise ValueError(f"dropout geÃ§ersiz: {dropout}. 0.0 ile 0.8 arasÄ±nda float olmalÄ±")
    
    # Performans uyarÄ±larÄ±
    if hidden_size > 256:
        logger.warning(f"   âš ï¸ BÃ¼yÃ¼k hidden_size ({hidden_size}) overfitting riskini artÄ±rabilir")
    
    if num_layers > 3:
        logger.warning(f"   âš ï¸ Ã‡ok katman ({num_layers}) gradient vanishing problemine yol aÃ§abilir")
    
    logger.info(f"   âœ… LSTM konfigÃ¼rasyonu doÄŸrulandÄ±")
    return config


def get_model_info(model: ModelInstance) -> Dict[str, Any]:
    """
    FarklÄ± mimarilerde birleÅŸik model bilgisi al.
    
    Args:
        model: Model instance'Ä±
        
    Returns:
        Model bilgi sÃ¶zlÃ¼ÄŸÃ¼
    """
    try:
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # Model tipine gÃ¶re detaylar
        model_type = type(model).__name__
        
        info = {
            'model_type': model_type,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'non_trainable_parameters': total_params - trainable_params,
            'device': str(next(model.parameters()).device),
        }
        
        # Model-specific bilgiler
        if hasattr(model, 'hidden_size'):
            info['hidden_size'] = model.hidden_size
        if hasattr(model, 'num_layers'):
            info['num_layers'] = model.num_layers
        if hasattr(model, 'd_model'):
            info['d_model'] = model.d_model
        if hasattr(model, 'nhead'):
            info['nhead'] = model.nhead
        if hasattr(model, 'lstm_hidden'):
            info['lstm_hidden'] = model.lstm_hidden
        
        return info
        
    except Exception as e:
        logger.error(f"   âŒ Model bilgisi alÄ±namadÄ±: {e}")
        return {
            'model_type': type(model).__name__,
            'total_parameters': 0,
            'trainable_parameters': 0,
            'error': str(e)
        }


def get_model_complexity_score(model: ModelInstance) -> float:
    """
    Model karmaÅŸÄ±klÄ±k skorunu hesapla.
    
    Args:
        model: Model instance'Ä±
        
    Returns:
        KarmaÅŸÄ±klÄ±k skoru (million parametreler cinsinden)
    """
    try:
        total_params = sum(p.numel() for p in model.parameters())
        complexity = total_params / 1e6  # Million parametreler
        
        return complexity
        
    except Exception as e:
        logger.error(f"   âŒ KarmaÅŸÄ±klÄ±k skoru hesaplanamadÄ±: {e}")
        return 1.0


def suggest_training_params(model: ModelInstance) -> Dict[str, Any]:
    """
    Model kompleksitesine gÃ¶re eÄŸitim parametreleri Ã¶ner.
    
    Args:
        model: Model instance'Ä±
        
    Returns:
        Ã–nerilen eÄŸitim parametreleri
    """
    try:
        complexity = get_model_complexity_score(model)
        info = get_model_info(model)
        
        logger.info(f"ğŸ¯ EÄŸitim parametreleri Ã¶neriliyor...")
        logger.info(f"   Model karmaÅŸÄ±klÄ±ÄŸÄ±: {complexity:.2f}M parameters")
        
        # Hibrit model iÃ§in Ã¶neriler
        if HYBRID_MODEL_AVAILABLE and isinstance(model, HybridLSTMTransformer):
            # Hibrit model detaylarÄ±
            lstm_hidden = getattr(model, 'lstm_hidden', 96)
            d_model = getattr(model, 'd_model', 512)
            num_layers = getattr(model, 'num_layers', 4)
            
            logger.info(f"   Hibrit model detaylarÄ±: LSTM hidden={lstm_hidden}, d_model={d_model}, layers={num_layers}")
            
            # KarmaÅŸÄ±klÄ±ÄŸa ve mimari detaylarÄ±na gÃ¶re Ã¶neri
            if complexity < 4.0 and num_layers <= 4:
                base_lr = 7e-4
                batch_size = 48
                warmup = 1200
            elif complexity < 10.0 or num_layers <= 6:
                base_lr = 3e-4
                batch_size = 32
                warmup = 2500
            else:
                base_lr = 1e-4
                batch_size = 16
                warmup = 4000
            
            # LSTM hidden size'a gÃ¶re ayarlama
            if lstm_hidden > 128:
                base_lr *= 0.85
                logger.info(f"   ğŸ“‰ BÃ¼yÃ¼k LSTM hidden_size nedeniyle LR dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # d_model'e gÃ¶re batch size ayarlamasÄ±
            if d_model > 512:
                batch_size = max(8, batch_size // 2)
                logger.info(f"   ğŸ“¦ BÃ¼yÃ¼k d_model nedeniyle batch_size dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            suggestions = {
                'learning_rate': base_lr,
                'batch_size': batch_size,
                'warmup_steps': warmup,
                'weight_decay': 0.01,
                'scheduler': 'CosineAnnealingWarmRestarts',
                'optimizer': 'AdamW',
                'gradient_clip': 0.3,
                'model_type': 'hybrid_lstm_transformer'
            }
        
        elif TRANSFORMER_AVAILABLE and isinstance(model, EnhancedTransformer):
            # Enhanced Transformer iÃ§in Ã¶neriler
            d_model = getattr(model, 'd_model', 256)
            nhead = getattr(model, 'nhead', 8)
            num_layers = getattr(model, 'num_layers', 6)
            
            logger.info(f"   Enhanced Transformer detaylarÄ±: d_model={d_model}, heads={nhead}, layers={num_layers}")
            
            # KarmaÅŸÄ±klÄ±ÄŸa gÃ¶re Ã¶neri
            if complexity < 2.0:
                base_lr = 5e-4
                batch_size = 64
                warmup = 800
            elif complexity < 8.0:
                base_lr = 2e-4
                batch_size = 32
                warmup = 1600
            else:
                base_lr = 8e-5
                batch_size = 16
                warmup = 3200
            
            # Attention head'e gÃ¶re ayarlama
            if nhead > 12:
                base_lr *= 0.8
                logger.info(f"   ğŸ“‰ Ã‡ok attention head nedeniyle LR dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            suggestions = {
                'learning_rate': base_lr,
                'batch_size': batch_size,
                'warmup_steps': warmup,
                'weight_decay': 0.01,
                'scheduler': 'OneCycleLR',
                'optimizer': 'AdamW',
                'gradient_clip': 1.0,
                'model_type': 'enhanced_transformer'
            }
        
        else:
            # LSTM iÃ§in Ã¶neriler
            num_layers = getattr(model, 'num_layers', 2)
            hidden_size = getattr(model, 'hidden_size', 64)
            
            logger.info(f"   LSTM detaylarÄ±: layers={num_layers}, hidden_size={hidden_size}")
            
            # KarmaÅŸÄ±klÄ±ÄŸa ve mimari detaylarÄ±na gÃ¶re Ã¶neri
            if complexity < 1.0 and num_layers <= 2:
                base_lr = 1e-3
                batch_size = 128
            elif complexity < 3.0 or num_layers <= 3:
                base_lr = 8e-4
                batch_size = 64
            else:
                base_lr = 5e-4
                batch_size = 32
            
            # Hidden size'a gÃ¶re ayarlama
            if hidden_size > 128:
                base_lr *= 0.8
                logger.info(f"   ğŸ“‰ BÃ¼yÃ¼k hidden_size nedeniyle LR dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # Katman sayÄ±sÄ±na gÃ¶re weight decay ayarlamasÄ±
            weight_decay = 0.0001 if num_layers <= 2 else 0.001
            
            suggestions = {
                'learning_rate': base_lr,
                'batch_size': batch_size,
                'weight_decay': weight_decay,
                'scheduler': 'ReduceLROnPlateau',
                'optimizer': 'AdamW',
                'gradient_clip': 0.5,
                'patience': 5,
                'model_type': 'lstm'
            }
        
        logger.info(f"   âœ… EÄŸitim parametreleri Ã¶nerildi: LR={suggestions['learning_rate']:.2e}, BS={suggestions['batch_size']}")
        
        return suggestions
        
    except Exception as e:
        logger.error(f"   âŒ Parametre Ã¶nerisi oluÅŸturulamadÄ±: {e}")
        raise RuntimeError(f"EÄŸitim parametresi Ã¶nerisi hatasÄ±: {e}") from e


def get_supported_models() -> List[str]:
    """
    Desteklenen model tiplerinin listesini dÃ¶ndÃ¼r.
    
    Returns:
        Desteklenen model tipleri listesi
    """
    return SUPPORTED_MODELS.copy()


def validate_model_compatibility(model_type: str, config: ModelConfig) -> Tuple[bool, List[str]]:
    """
    Model tipi ve konfigÃ¼rasyon uyumluluÄŸunu kontrol et.
    
    Args:
        model_type: Model tipi
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        (uyumlu_mu, uyarÄ±_listesi) tuple'Ä±
    """
    warnings = []
    is_compatible = True
    
    try:
        if model_type.lower() in TRANSFORMER_ALIASES:
            validate_transformer_config(config)
        elif model_type.lower() in LSTM_ALIASES:
            validate_lstm_config(config)
        elif model_type.lower() in HYBRID_ALIASES:
            validate_hybrid_config(config) if HYBRID_MODEL_AVAILABLE else warnings.append("Hibrit model mevcut deÄŸil")
        else:
            warnings.append(f"Bilinmeyen model tipi: {model_type}")
            is_compatible = False
            
    except ValueError as e:
        warnings.append(str(e))
        is_compatible = False
    
    return is_compatible, warnings


# Test fonksiyonu da ekleyelim
def test_hybrid_model_creation():
    """Test hibrit model oluÅŸturma"""
    if not HYBRID_MODEL_AVAILABLE:
        logger.warning("âš ï¸ Hibrit model mevcut deÄŸil, test atlanÄ±yor")
        return False
        
    try:
        from .. import config
        
        # Test konfigÃ¼rasyonu
        test_config = config.get_model_config('hybrid_lstm_transformer', 'three_class')
        device = torch.device('cpu')
        n_features = 20
        
        logger.info(f"ğŸ§ª Hibrit model testi baÅŸlÄ±yor...")
        logger.info(f"   Config: {test_config}")
        
        model = create_model('hybrid_lstm_transformer', test_config, n_features, device)
        
        # Test input
        batch_size, seq_len = 8, 50
        test_input = torch.randn(batch_size, seq_len, n_features)
        
        logger.info(f"   Test input shape: {test_input.shape}")
        
        with torch.no_grad():
            output = model(test_input)
            
        expected_classes = 3 if test_config.get('target_mode') == 'three_class' else 2
        assert output.shape == (batch_size, expected_classes), f"Wrong output shape: {output.shape}"
        
        logger.info(f"âœ… Hibrit model testi baÅŸarÄ±lÄ±!")
        logger.info(f"   Input: {test_input.shape} â†’ Output: {output.shape}")
        logger.info(f"   Model parametreleri: {sum(p.numel() for p in model.parameters()):,}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Hibrit model testi baÅŸarÄ±sÄ±z: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return False


__all__ = [
    'create_model',
    'get_model_info',
    'validate_transformer_config',
    'validate_enhanced_transformer_config',
    'validate_lstm_config',
    'validate_hybrid_config' if HYBRID_MODEL_AVAILABLE else 'validate_lstm_config',
    'get_model_complexity_score',
    'suggest_training_params',
    'get_supported_models',
    'validate_model_compatibility',
    'test_hybrid_model_creation',
    'SUPPORTED_MODELS',
    'ModelConfig',
    'TransformerConfig', 
    'LSTMConfig',
    'HybridConfig',
    'ModelInstance'
]
