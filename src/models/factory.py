"""
GeliÅŸtirilmiÅŸ model factory for creating different model architectures.

Bu modÃ¼l, LSTM, EnhancedTransformer, HybridLSTMTransformer ve diÄŸer modelleri oluÅŸturmak iÃ§in birleÅŸik arayÃ¼z saÄŸlar.
Model seÃ§imi, parametre validasyonu, cihaz yerleÅŸtirme ve geliÅŸmiÅŸ hata yÃ¶netimi
ile kapsamlÄ± konfigÃ¼rasyon doÄŸrulamasÄ± iÃ§erir.
"""

import torch
import copy
import logging
import importlib
from typing import Dict, Any, Union, List, Tuple, Type

# Logger yapÄ±landÄ±rmasÄ± - EN BAÅTA TANIMLA
logger = logging.getLogger(__name__)

# LSTM modeli iÃ§in import (her zaman mevcut)
from .lstm import PairSpecificLSTM, create_model as create_lstm_model

# Transformer modeli iÃ§in koÅŸullu import
TRANSFORMER_AVAILABLE = False
ENHANCED_TRANSFORMER_V2_AVAILABLE = False
HYBRID_MODEL_AVAILABLE = False
EnhancedTransformer = None
TransformerClassifier = None
create_transformer_model = None
create_enhanced_transformer = None
EnhancedTransformerV2 = None
create_enhanced_transformer_v2 = None
HybridLSTMTransformer = None

# Transformer modÃ¼lÃ¼nÃ¼ dinamik olarak yÃ¼kleme
try:
    transformer_module = importlib.import_module('.transformer_model', package='src.models')
    
    # EnhancedTransformer var mÄ±?
    if hasattr(transformer_module, 'EnhancedTransformer'):
        from .transformer_model import EnhancedTransformer, create_enhanced_transformer
        TRANSFORMER_AVAILABLE = True
        logger.info("âœ… EnhancedTransformer import baÅŸarÄ±lÄ±")
    
    # TransformerClassifier var mÄ±?
    if hasattr(transformer_module, 'TransformerClassifier'):
        from .transformer_model import TransformerClassifier, create_transformer_model
        logger.info("âœ… TransformerClassifier import baÅŸarÄ±lÄ±")
    else:
        logger.warning("âš ï¸ TransformerClassifier bulunamadÄ±")
        
except ImportError as e:
    logger.warning(f"âš ï¸ Transformer model import hatasÄ±: {e}")
except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata (transformer_model): {e}")

# Enhanced Transformer V2 iÃ§in import
try:
    enhanced_module = importlib.import_module('.enhanced_transformer', package='src.models')
    
    if hasattr(enhanced_module, 'EnhancedTransformer'):
        from .enhanced_transformer import EnhancedTransformer as EnhancedTransformerV2
        from .enhanced_transformer import create_enhanced_transformer as create_enhanced_transformer_v2
        ENHANCED_TRANSFORMER_V2_AVAILABLE = True
        logger.info("âœ… Enhanced Transformer V2 import baÅŸarÄ±lÄ±")
        
except ImportError as e:
    logger.warning(f"âš ï¸ Enhanced Transformer V2 import hatasÄ±: {e}")
except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata (enhanced_transformer): {e}")

# Hibrit LSTM-Transformer modeli iÃ§in import
try:
    hybrid_module = importlib.import_module('.hybrid_model', package='src.models')
    
    if hasattr(hybrid_module, 'HybridLSTMTransformer'):
        from .hybrid_model import HybridLSTMTransformer
        HYBRID_MODEL_AVAILABLE = True
        logger.info("âœ… HybridLSTMTransformer import baÅŸarÄ±lÄ±")
        
except ImportError as e:
    logger.warning(f"âš ï¸ Hybrid model import hatasÄ±: {e}")
except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata (hybrid_model): {e}")

# Model tipi sabitleri - GÃœVENLÄ° HALE GETÄ°R
SUPPORTED_MODELS = ['lstm', 'pairspecificlstm']

# Mevcut modellere gÃ¶re desteklenen tipleri ekle
if TRANSFORMER_AVAILABLE:
    SUPPORTED_MODELS.extend(['transformer', 'enhanced_transformer'])
    logger.info("âœ… Transformer modelleri desteklenen listeye eklendi")

if ENHANCED_TRANSFORMER_V2_AVAILABLE:
    SUPPORTED_MODELS.append('enhanced_transformer_v2')
    logger.info("âœ… Enhanced Transformer V2 desteklenen listeye eklendi")

if HYBRID_MODEL_AVAILABLE:
    SUPPORTED_MODELS.append('hybrid_lstm_transformer')
    logger.info("âœ… Hibrit LSTM-Transformer desteklenen listeye eklendi")

logger.info(f"ğŸ“‹ Desteklenen modeller: {SUPPORTED_MODELS}")

# Model alias'larÄ±
LSTM_ALIASES = ['lstm', 'pairspecificlstm']
TRANSFORMER_ALIASES = ['transformer', 'enhanced_transformer'] if TRANSFORMER_AVAILABLE else []
ENHANCED_TRANSFORMER_V2_ALIASES = ['enhanced_transformer_v2'] if ENHANCED_TRANSFORMER_V2_AVAILABLE else []
HYBRID_ALIASES = ['hybrid_lstm_transformer'] if HYBRID_MODEL_AVAILABLE else []

# ModelInstance tipini gÃ¼venli hale getir
available_types = [PairSpecificLSTM]

if TRANSFORMER_AVAILABLE:
    if EnhancedTransformer:
        available_types.append(EnhancedTransformer)
    if TransformerClassifier:
        available_types.append(TransformerClassifier)

if ENHANCED_TRANSFORMER_V2_AVAILABLE and EnhancedTransformerV2:
    available_types.append(EnhancedTransformerV2)

if HYBRID_MODEL_AVAILABLE and HybridLSTMTransformer:
    available_types.append(HybridLSTMTransformer)

if len(available_types) == 0:
    logger.error("âš ï¸ HiÃ§bir model tipi kullanÄ±lamÄ±yor! Sadece LSTM kullanÄ±lacak.")
    available_types = [PairSpecificLSTM]

ModelInstance = Union[tuple(available_types)]

# Tip alias'larÄ± (daha aÃ§Ä±klayÄ±cÄ± tip notasyonlarÄ± iÃ§in)
ModelConfig = Dict[str, Any]
TransformerConfig = Dict[str, Any]
LSTMConfig = Dict[str, Any]
HybridConfig = Dict[str, Any]


def create_model(
    model_type: str,
    config: ModelConfig, 
    n_features: int, 
    device: torch.device
) -> ModelInstance:
    """
    Tip ve konfigÃ¼rasyona gÃ¶re model oluÅŸturan factory fonksiyonu.
    
    Args:
        model_type: OluÅŸturulacak model tipi ('lstm', 'transformer', 'hybrid_lstm_transformer', ...)
        config: Model konfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        n_features: GiriÅŸ Ã¶zellik sayÄ±sÄ±
        device: Model oluÅŸturulacak cihaz
        
    Returns:
        BaÅŸlatÄ±lmÄ±ÅŸ model instance'Ä±
        
    Raises:
        ValueError: Model tipi desteklenmiyorsa veya konfigÃ¼rasyon geÃ§ersizse
        RuntimeError: Model oluÅŸturma sÄ±rasÄ±nda beklenmeyen hata
    """
    model_type_normalized = model_type.lower().strip()
    
    logger.info(f"ğŸ­ Model factory: {model_type_normalized} modeli oluÅŸturuluyor...")
    logger.info(f"   GiriÅŸ Ã¶zellikleri: {n_features}")
    logger.info(f"   Cihaz: {device}")
    
    # Model tipi validasyonu
    if model_type_normalized not in SUPPORTED_MODELS:
        raise ValueError(
            f"Desteklenmeyen model tipi: '{model_type}'. "
            f"Desteklenen modeller: {SUPPORTED_MODELS}"
        )
    
    try:
        if model_type_normalized in LSTM_ALIASES:
            # LSTM konfigÃ¼rasyonu validasyonu
            validated_config = validate_lstm_config(config)
            model = create_lstm_model(validated_config, n_features, device)
            logger.info(f"   âœ… LSTM modeli baÅŸarÄ±yla oluÅŸturuldu")
            
        elif model_type_normalized in TRANSFORMER_ALIASES:
            if not TRANSFORMER_AVAILABLE:
                raise RuntimeError("Transformer modeli desteklenmiyor (import hatasÄ±)")
                
            # Transformer konfigÃ¼rasyonu validasyonu
            validated_config = validate_transformer_config(config)
            
            if model_type_normalized == 'transformer' and create_transformer_model:
                model = create_transformer_model(validated_config, n_features, device)
                logger.info(f"   âœ… Transformer modeli baÅŸarÄ±yla oluÅŸturuldu")
            elif model_type_normalized == 'enhanced_transformer' and create_enhanced_transformer:
                model = create_enhanced_transformer(validated_config, n_features, device)
                logger.info(f"   âœ… EnhancedTransformer modeli baÅŸarÄ±yla oluÅŸturuldu")
            else:
                raise RuntimeError(f"Ä°stenen model tipi desteklenmiyor: {model_type}")
                
        elif model_type_normalized in ENHANCED_TRANSFORMER_V2_ALIASES:
            if not ENHANCED_TRANSFORMER_V2_AVAILABLE or not create_enhanced_transformer_v2:
                raise RuntimeError("Enhanced Transformer V2 modeli desteklenmiyor (import hatasÄ±)")
                
            # EnhancedTransformer V2 konfigÃ¼rasyonu validasyonu
            validated_config = validate_enhanced_transformer_config(config)
            model = create_enhanced_transformer_v2(validated_config, n_features, device)
            logger.info(f"   âœ… EnhancedTransformer V2 modeli baÅŸarÄ±yla oluÅŸturuldu")
        
        elif model_type_normalized in HYBRID_ALIASES:
            if not HYBRID_MODEL_AVAILABLE or not HybridLSTMTransformer:
                raise RuntimeError("Hibrit LSTM-Transformer modeli desteklenmiyor (import hatasÄ±)")
                
            # Hibrit model konfigÃ¼rasyonu validasyonu
            validated_config = validate_hybrid_config(config)
            model = create_hybrid_model(validated_config, n_features, device)
            logger.info(f"   âœ… Hibrit LSTM-Transformer modeli baÅŸarÄ±yla oluÅŸturuldu")
        
        else:
            # Bu duruma teorik olarak gelmemeli ama gÃ¼venlik iÃ§in
            raise ValueError(f"Model tipi '{model_type}' iÅŸlenemiyor")
        
        # Model bilgilerini logla
        model_info = get_model_info(model)
        logger.info(f"   ğŸ“Š Model parametreleri: {model_info['total_parameters']:,}")
        
        return model
        
    except ValueError as e:
        logger.error(f"   âŒ Model oluÅŸturma hatasÄ±: {e}")
        raise
    except Exception as e:
        logger.error(f"   âŒ Beklenmeyen hata: {e}")
        raise RuntimeError(f"Model oluÅŸturma baÅŸarÄ±sÄ±z: {e}") from e


def create_hybrid_model(config: HybridConfig, n_features: int, device: torch.device) -> HybridLSTMTransformer:
    """
    Hibrit LSTM-Transformer modeli oluÅŸturur
    
    Args:
        config: Hibrit model konfigÃ¼rasyonu
        n_features: GiriÅŸ Ã¶zellik sayÄ±sÄ±
        device: Modelin yerleÅŸtirileceÄŸi cihaz
        
    Returns:
        OluÅŸturulan hibrit model instance'Ä±
    """
    # Hibrit-specific config
    hybrid_config = config.get('hybrid', {})
    
    # Parametreleri Ã§Ä±kar veya default deÄŸerler kullan
    lstm_hidden = hybrid_config.get('lstm_hidden', 96)
    d_model = hybrid_config.get('d_model', 512)
    nhead = hybrid_config.get('nhead', 8)
    num_layers = hybrid_config.get('num_layers', 4)
    dropout = hybrid_config.get('dropout', 0.1)
    
    model = HybridLSTMTransformer(
        input_dim=n_features,
        lstm_hidden=lstm_hidden,
        d_model=d_model,
        nhead=nhead,
        num_layers=num_layers,
        dropout=dropout
    ).to(device)
    
    logger.info(f"   âœ… Hibrit LSTM-Transformer modeli oluÅŸturuldu")
    logger.info(f"      LSTM hidden: {lstm_hidden}, Transformer d_model: {d_model}")
    
    return model


def validate_hybrid_config(config: HybridConfig) -> HybridConfig:
    """
    Hibrit model konfigÃ¼rasyon parametrelerini doÄŸrula ve ayarla.
    
    Args:
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        DoÄŸrulanmÄ±ÅŸ konfigÃ¼rasyon
        
    Raises:
        ValueError: KonfigÃ¼rasyon geÃ§ersizse
    """
    config = copy.deepcopy(config)
    hybrid_config = config.get('hybrid', {})
    
    lstm_hidden = hybrid_config.get('lstm_hidden', 96)
    d_model = hybrid_config.get('d_model', 512)
    nhead = hybrid_config.get('nhead', 8)
    num_layers = hybrid_config.get('num_layers', 4)
    dropout = hybrid_config.get('dropout', 0.1)
    
    logger.info(f"ğŸ” Hibrit model konfigÃ¼rasyonu doÄŸrulanÄ±yor...")
    logger.info(f"   LSTM hidden: {lstm_hidden}, d_model: {d_model}, layers: {num_layers}")
    
    # LSTM hidden size validasyonu
    if not isinstance(lstm_hidden, int) or lstm_hidden < 16 or lstm_hidden > 512:
        raise ValueError(
            f"LSTM hidden_size geÃ§ersiz: {lstm_hidden}. "
            f"16 ile 512 arasÄ±nda integer olmalÄ±"
        )
    
    # Transformer d_model validasyonu
    if not isinstance(d_model, int) or d_model < 64 or d_model > 1024:
        raise ValueError(
            f"d_model geÃ§ersiz: {d_model}. "
            f"64 ile 1024 arasÄ±nda integer olmalÄ±"
        )
    
    # nhead validasyonu
    if not isinstance(nhead, int) or nhead < 1 or nhead > 16:
        raise ValueError(
            f"nhead geÃ§ersiz: {nhead}. "
            f"1 ile 16 arasÄ±nda integer olmalÄ±"
        )
    
    # d_model ve nhead uyumluluÄŸu
    if d_model % nhead != 0:
        logger.warning(f"   âš ï¸ d_model ({d_model}) nhead ({nhead}) ile bÃ¶lÃ¼nemiyor")
        
        # Otomatik dÃ¼zeltme denemesi
        valid_heads = [h for h in [1, 2, 4, 8, 16] if d_model % h == 0 and h <= d_model]
        if valid_heads:
            old_nhead = nhead
            nhead = max([h for h in valid_heads if h <= nhead]) or valid_heads[-1]
            hybrid_config['nhead'] = nhead
            logger.warning(f"   ğŸ”§ nhead otomatik dÃ¼zeltildi: {old_nhead} â†’ {nhead}")
        else:
            raise ValueError(
                f"d_model ({d_model}) nhead ({nhead}) ile bÃ¶lÃ¼nebilir olmalÄ±. "
                f"d_model % nhead == 0 koÅŸulu saÄŸlanmalÄ±. "
                f"GeÃ§erli nhead deÄŸerleri: {[h for h in [1, 2, 4, 8, 16] if d_model % h == 0]}"
            )
    
    # num_layers validasyonu
    if not isinstance(num_layers, int) or num_layers < 1 or num_layers > 8:
        raise ValueError(
            f"num_layers geÃ§ersiz: {num_layers}. "
            f"1 ile 8 arasÄ±nda integer olmalÄ±"
        )
    
    # dropout validasyonu
    if not isinstance(dropout, (int, float)) or dropout < 0.0 or dropout > 0.5:
        raise ValueError(
            f"dropout geÃ§ersiz: {dropout}. "
            f"0.0 ile 0.5 arasÄ±nda float olmalÄ±"
        )
    
    # Performans uyarÄ±larÄ±
    if lstm_hidden > 256:
        logger.warning(f"   âš ï¸ BÃ¼yÃ¼k LSTM hidden_size ({lstm_hidden}) overfitting riskini artÄ±rabilir")
    
    if d_model > 512:
        logger.warning(f"   âš ï¸ BÃ¼yÃ¼k d_model ({d_model}) Ã¶nemli GPU belleÄŸi gerektirebilir")
    
    if num_layers > 6:
        logger.warning(f"   âš ï¸ Ã‡ok katman ({num_layers}) eÄŸitimi yavaÅŸlatabilir")
    
    logger.info(f"   âœ… Hibrit model konfigÃ¼rasyonu doÄŸrulandÄ±")
    
    return config


def validate_transformer_config(config: TransformerConfig) -> TransformerConfig:
    # ... (Mevcut kod aynÄ± kalÄ±r, deÄŸiÅŸiklik yok) ...

def validate_enhanced_transformer_config(config: TransformerConfig) -> TransformerConfig:
    # ... (Mevcut kod aynÄ± kalÄ±r, deÄŸiÅŸiklik yok) ...

def validate_lstm_config(config: LSTMConfig) -> LSTMConfig:
    # ... (Mevcut kod aynÄ± kalÄ±r, deÄŸiÅŸiklik yok) ...

def get_model_info(model: ModelInstance) -> Dict[str, Any]:
    # ... (Mevcut kod aynÄ± kalÄ±r, deÄŸiÅŸiklik yok) ...

def get_model_complexity_score(model: ModelInstance) -> float:
    # ... (Mevcut kod aynÄ± kalÄ±r, deÄŸiÅŸiklik yok) ...

def suggest_training_params(model: ModelInstance) -> Dict[str, Any]:
    try:
        complexity = get_model_complexity_score(model)
        info = get_model_info(model)
        
        logger.info(f"ğŸ¯ EÄŸitim parametreleri Ã¶neriliyor...")
        logger.info(f"   Model karmaÅŸÄ±klÄ±ÄŸÄ±: {complexity}")
        
        # YENÄ°: Hibrit model iÃ§in Ã¶neriler
        if HYBRID_MODEL_AVAILABLE and isinstance(model, HybridLSTMTransformer):
            # Hibrit model detaylarÄ±
            lstm_hidden = getattr(model, 'lstm_hidden_size', 96)
            d_model = getattr(model, 'd_model', 512)
            num_layers = getattr(model, 'num_layers', 4)
            
            logger.info(f"   Hibrit model detaylarÄ±: LSTM hidden={lstm_hidden}, d_model={d_model}, layers={num_layers}")
            
            # KarmaÅŸÄ±klÄ±ÄŸa ve mimari detaylarÄ±na gÃ¶re Ã¶neri
            if complexity < 4.0 and num_layers <= 4:
                base_lr = 7e-5
                batch_size = 48
                warmup = 1200
            elif complexity < 10.0 or num_layers <= 6:
                base_lr = 3e-5
                batch_size = 32
                warmup = 2500
            else:
                base_lr = 1e-5
                batch_size = 16
                warmup = 4000
            
            # LSTM hidden size'a gÃ¶re ayarlama
            if lstm_hidden > 128:
                base_lr *= 0.85
                logger.info(f"   ğŸ“‰ BÃ¼yÃ¼k LSTM hidden_size nedeniyle LR dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # d_model'e gÃ¶re batch size ayarlamasÄ±
            if d_model > 512:
                batch_size = max(8, batch_size // 2)
                logger.info(f"   ğŸ“¦ BÃ¼yÃ¼k d_model nedeniyle batch_size dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            suggestions = {
                'learning_rate': base_lr,
                'batch_size': batch_size,
                'warmup_steps': warmup,
                'weight_decay': 0.01,
                'scheduler': 'CosineAnnealingWarmRestarts',
                'optimizer': 'AdamW',
                'gradient_clip': 0.3,
                'model_type': 'hybrid_lstm_transformer'
            }
        
        elif TRANSFORMER_AVAILABLE and isinstance(model, EnhancedTransformer):
            # ... (Mevcut kod aynÄ± kalÄ±r) ...
        elif ENHANCED_TRANSFORMER_V2_AVAILABLE and isinstance(model, EnhancedTransformerV2):
            # ... (Mevcut kod aynÄ± kalÄ±r) ...
        elif TRANSFORMER_AVAILABLE and isinstance(model, TransformerClassifier):
            # ... (Mevcut kod aynÄ± kalÄ±r) ...
        else:
            # ... (Mevcut kod aynÄ± kalÄ±r) ...
        
        logger.info(f"   âœ… EÄŸitim parametreleri Ã¶nerildi: LR={suggestions['learning_rate']:.2e}, BS={suggestions['batch_size']}")
        return suggestions
        
    except Exception as e:
        logger.error(f"   âŒ Parametre Ã¶nerisi oluÅŸturulamadÄ±: {e}")
        raise RuntimeError(f"EÄŸitim parametresi Ã¶nerisi hatasÄ±: {e}") from e


def get_supported_models() -> List[str]:
    # ... (Mevcut kod aynÄ± kalÄ±r, deÄŸiÅŸiklik yok) ...

def validate_model_compatibility(model_type: str, config: ModelConfig) -> Tuple[bool, List[str]]:
    """
    Model tipi ve konfigÃ¼rasyon uyumluluÄŸunu kontrol et.
    
    Args:
        model_type: Model tipi
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        (uyumlu_mu, uyarÄ±_listesi) tuple'Ä±
    """
    warnings = []
    is_compatible = True
    
    try:
        if model_type.lower() in TRANSFORMER_ALIASES:
            validate_transformer_config(config)
        elif model_type.lower() in ENHANCED_TRANSFORMER_V2_ALIASES:
            validate_enhanced_transformer_config(config)
        elif model_type.lower() in LSTM_ALIASES:
            validate_lstm_config(config)
        elif model_type.lower() in HYBRID_ALIASES:
            validate_hybrid_config(config)  # YENÄ°: Hibrit model validasyonu
        else:
            warnings.append(f"Bilinmeyen model tipi: {model_type}")
            is_compatible = False
            
    except ValueError as e:
        warnings.append(str(e))
        is_compatible = False
    
    return is_compatible, warnings


__all__ = [
    'create_model',
    'get_model_info',
    'validate_transformer_config',
    'validate_enhanced_transformer_config',
    'validate_lstm_config',
    'validate_hybrid_config',  # YENÄ°
    'get_model_complexity_score',
    'suggest_training_params',
    'get_supported_models',
    'validate_model_compatibility',
    'SUPPORTED_MODELS',
    'ModelConfig',
    'TransformerConfig', 
    'LSTMConfig',
    'HybridConfig',  # YENÄ°
    'ModelInstance'
]
