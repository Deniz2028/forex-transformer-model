"""
GeliÅŸtirilmiÅŸ model factory for creating different model architectures.

Bu modÃ¼l, LSTM, EnhancedTransformer ve diÄŸer modelleri oluÅŸturmak iÃ§in birleÅŸik arayÃ¼z saÄŸlar.
Model seÃ§imi, parametre validasyonu, cihaz yerleÅŸtirme ve geliÅŸmiÅŸ hata yÃ¶netimi
ile kapsamlÄ± konfigÃ¼rasyon doÄŸrulamasÄ± iÃ§erir.
"""

import torch
import copy
import logging
import importlib
from typing import Dict, Any, Union, List, Tuple, Type

# Logger yapÄ±landÄ±rmasÄ± - EN BAÅTA TANIMLA
logger = logging.getLogger(__name__)

# LSTM modeli iÃ§in import (her zaman mevcut)
from .lstm import PairSpecificLSTM, create_model as create_lstm_model

# Transformer modeli iÃ§in koÅŸullu import
TRANSFORMER_AVAILABLE = False
ENHANCED_TRANSFORMER_V2_AVAILABLE = False
EnhancedTransformer = None
TransformerClassifier = None
create_transformer_model = None
create_enhanced_transformer = None
EnhancedTransformerV2 = None
create_enhanced_transformer_v2 = None

# Transformer modÃ¼lÃ¼nÃ¼ dinamik olarak yÃ¼kleme
try:
    transformer_module = importlib.import_module('.transformer_model', package='src.models')
    
    # EnhancedTransformer var mÄ±?
    if hasattr(transformer_module, 'EnhancedTransformer'):
        from .transformer_model import EnhancedTransformer, create_enhanced_transformer
        TRANSFORMER_AVAILABLE = True
        logger.info("âœ… EnhancedTransformer import baÅŸarÄ±lÄ±")
    
    # TransformerClassifier var mÄ±?
    if hasattr(transformer_module, 'TransformerClassifier'):
        from .transformer_model import TransformerClassifier, create_transformer_model
        logger.info("âœ… TransformerClassifier import baÅŸarÄ±lÄ±")
    else:
        logger.warning("âš ï¸ TransformerClassifier bulunamadÄ±")
        
except ImportError as e:
    logger.warning(f"âš ï¸ Transformer model import hatasÄ±: {e}")
except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata (transformer_model): {e}")

# Enhanced Transformer V2 iÃ§in import
try:
    enhanced_module = importlib.import_module('.enhanced_transformer', package='src.models')
    
    if hasattr(enhanced_module, 'EnhancedTransformer'):
        from .enhanced_transformer import EnhancedTransformer as EnhancedTransformerV2
        from .enhanced_transformer import create_enhanced_transformer as create_enhanced_transformer_v2
        ENHANCED_TRANSFORMER_V2_AVAILABLE = True
        logger.info("âœ… Enhanced Transformer V2 import baÅŸarÄ±lÄ±")
        
except ImportError as e:
    logger.warning(f"âš ï¸ Enhanced Transformer V2 import hatasÄ±: {e}")
except Exception as e:
    logger.error(f"âŒ Beklenmeyen hata (enhanced_transformer): {e}")

# Model tipi sabitleri - GÃœVENLÄ° HALE GETÄ°R
SUPPORTED_MODELS = ['lstm', 'pairspecificlstm']

# Mevcut modellere gÃ¶re desteklenen tipleri ekle
if TRANSFORMER_AVAILABLE:
    SUPPORTED_MODELS.extend(['transformer', 'enhanced_transformer'])
    logger.info("âœ… Transformer modelleri desteklenen listeye eklendi")

if ENHANCED_TRANSFORMER_V2_AVAILABLE:
    SUPPORTED_MODELS.append('enhanced_transformer_v2')
    logger.info("âœ… Enhanced Transformer V2 desteklenen listeye eklendi")

logger.info(f"ğŸ“‹ Desteklenen modeller: {SUPPORTED_MODELS}")

# Model alias'larÄ±
LSTM_ALIASES = ['lstm', 'pairspecificlstm']
TRANSFORMER_ALIASES = ['transformer', 'enhanced_transformer'] if TRANSFORMER_AVAILABLE else []
ENHANCED_TRANSFORMER_V2_ALIASES = ['enhanced_transformer_v2'] if ENHANCED_TRANSFORMER_V2_AVAILABLE else []

# ModelInstance tipini gÃ¼venli hale getir
available_types = [PairSpecificLSTM]

if TRANSFORMER_AVAILABLE:
    if EnhancedTransformer:
        available_types.append(EnhancedTransformer)
    if TransformerClassifier:
        available_types.append(TransformerClassifier)

if ENHANCED_TRANSFORMER_V2_AVAILABLE and EnhancedTransformerV2:
    available_types.append(EnhancedTransformerV2)

if len(available_types) == 0:
    logger.error("âš ï¸ HiÃ§bir model tipi kullanÄ±lamÄ±yor! Sadece LSTM kullanÄ±lacak.")
    available_types = [PairSpecificLSTM]

ModelInstance = Union[tuple(available_types)]

# Tip alias'larÄ± (daha aÃ§Ä±klayÄ±cÄ± tip notasyonlarÄ± iÃ§in)
ModelConfig = Dict[str, Any]
TransformerConfig = Dict[str, Any]
LSTMConfig = Dict[str, Any]


def create_model(
    model_type: str,
    config: ModelConfig, 
    n_features: int, 
    device: torch.device
) -> ModelInstance:
    """
    Tip ve konfigÃ¼rasyona gÃ¶re model oluÅŸturan factory fonksiyonu.
    
    Args:
        model_type: OluÅŸturulacak model tipi ('lstm', 'transformer', 'enhanced_transformer', 'enhanced_transformer_v2')
        config: Model konfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        n_features: GiriÅŸ Ã¶zellik sayÄ±sÄ±
        device: Model oluÅŸturulacak cihaz
        
    Returns:
        BaÅŸlatÄ±lmÄ±ÅŸ model instance'Ä±
        
    Raises:
        ValueError: Model tipi desteklenmiyorsa veya konfigÃ¼rasyon geÃ§ersizse
        RuntimeError: Model oluÅŸturma sÄ±rasÄ±nda beklenmeyen hata
    """
    model_type_normalized = model_type.lower().strip()
    
    logger.info(f"ğŸ­ Model factory: {model_type_normalized} modeli oluÅŸturuluyor...")
    logger.info(f"   GiriÅŸ Ã¶zellikleri: {n_features}")
    logger.info(f"   Cihaz: {device}")
    
    # Model tipi validasyonu
    if model_type_normalized not in SUPPORTED_MODELS:
        raise ValueError(
            f"Desteklenmeyen model tipi: '{model_type}'. "
            f"Desteklenen modeller: {SUPPORTED_MODELS}"
        )
    
    try:
        if model_type_normalized in LSTM_ALIASES:
            # LSTM konfigÃ¼rasyonu validasyonu
            validated_config = validate_lstm_config(config)
            model = create_lstm_model(validated_config, n_features, device)
            logger.info(f"   âœ… LSTM modeli baÅŸarÄ±yla oluÅŸturuldu")
            
        elif model_type_normalized in TRANSFORMER_ALIASES:
            if not TRANSFORMER_AVAILABLE:
                raise RuntimeError("Transformer modeli desteklenmiyor (import hatasÄ±)")
                
            # Transformer konfigÃ¼rasyonu validasyonu
            validated_config = validate_transformer_config(config)
            
            if model_type_normalized == 'transformer' and create_transformer_model:
                model = create_transformer_model(validated_config, n_features, device)
                logger.info(f"   âœ… Transformer modeli baÅŸarÄ±yla oluÅŸturuldu")
            elif model_type_normalized == 'enhanced_transformer' and create_enhanced_transformer:
                model = create_enhanced_transformer(validated_config, n_features, device)
                logger.info(f"   âœ… EnhancedTransformer modeli baÅŸarÄ±yla oluÅŸturuldu")
            else:
                raise RuntimeError(f"Ä°stenen model tipi desteklenmiyor: {model_type}")
                
        elif model_type_normalized in ENHANCED_TRANSFORMER_V2_ALIASES:
            if not ENHANCED_TRANSFORMER_V2_AVAILABLE or not create_enhanced_transformer_v2:
                raise RuntimeError("Enhanced Transformer V2 modeli desteklenmiyor (import hatasÄ±)")
                
            # EnhancedTransformer V2 konfigÃ¼rasyonu validasyonu
            validated_config = validate_enhanced_transformer_config(config)
            model = create_enhanced_transformer_v2(validated_config, n_features, device)
            logger.info(f"   âœ… EnhancedTransformer V2 modeli baÅŸarÄ±yla oluÅŸturuldu")
        
        else:
            # Bu duruma teorik olarak gelmemeli ama gÃ¼venlik iÃ§in
            raise ValueError(f"Model tipi '{model_type}' iÅŸlenemiyor")
        
        # Model bilgilerini logla
        model_info = get_model_info(model)
        logger.info(f"   ğŸ“Š Model parametreleri: {model_info['total_parameters']:,}")
        
        return model
        
    except ValueError as e:
        logger.error(f"   âŒ Model oluÅŸturma hatasÄ±: {e}")
        raise
    except Exception as e:
        logger.error(f"   âŒ Beklenmeyen hata: {e}")
        raise RuntimeError(f"Model oluÅŸturma baÅŸarÄ±sÄ±z: {e}") from e


def validate_transformer_config(config: TransformerConfig) -> TransformerConfig:
    """
    Transformer konfigÃ¼rasyon parametrelerini doÄŸrula ve ayarla.
    
    Args:
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        DoÄŸrulanmÄ±ÅŸ konfigÃ¼rasyon
        
    Raises:
        ValueError: KonfigÃ¼rasyon geÃ§ersizse
    """
    # Orijinal sÃ¶zlÃ¼ÄŸÃ¼ bozmamak iÃ§in deep copy
    config = copy.deepcopy(config)
    
    transformer_config = config.get('transformer', {})
    
    d_model = transformer_config.get('d_model', 128)
    nhead = transformer_config.get('nhead', 8)
    num_layers = transformer_config.get('num_layers', 4)
    dim_feedforward = transformer_config.get('dim_feedforward', 256)
    dropout = config.get('model', {}).get('dropout_rate', 0.1)
    
    logger.info(f"ğŸ” Transformer konfigÃ¼rasyonu doÄŸrulanÄ±yor...")
    logger.info(f"   d_model: {d_model}, nhead: {nhead}, layers: {num_layers}")
    
    # d_model validasyonu
    if not isinstance(d_model, int) or d_model < 32 or d_model > 1024:
        raise ValueError(
            f"d_model geÃ§ersiz: {d_model}. "
            f"32 ile 1024 arasÄ±nda integer olmalÄ±"
        )
    
    # nhead validasyonu
    if not isinstance(nhead, int) or nhead < 1 or nhead > 32:
        raise ValueError(
            f"nhead geÃ§ersiz: {nhead}. "
            f"1 ile 32 arasÄ±nda integer olmalÄ±"
        )
    
    # d_model ve nhead uyumluluÄŸu
    if d_model % nhead != 0:
        logger.warning(f"   âš ï¸ d_model ({d_model}) nhead ({nhead}) ile bÃ¶lÃ¼nemiyor")
        
        # Otomatik dÃ¼zeltme denemesi
        valid_heads = [h for h in [1, 2, 4, 8, 16, 32] if d_model % h == 0 and h <= d_model]
        if valid_heads:
            old_nhead = nhead
            nhead = max([h for h in valid_heads if h <= nhead]) or valid_heads[-1]
            transformer_config['nhead'] = nhead
            logger.warning(f"   ğŸ”§ nhead otomatik dÃ¼zeltildi: {old_nhead} â†’ {nhead}")
        else:
            raise ValueError(
                f"d_model ({d_model}) nhead ({nhead}) ile bÃ¶lÃ¼nebilir olmalÄ±. "
                f"d_model % nhead == 0 koÅŸulu saÄŸlanmalÄ±. "
                f"GeÃ§erli nhead deÄŸerleri: {[h for h in [1, 2, 4, 8, 16, 32] if d_model % h == 0]}"
            )
    
    # num_layers validasyonu
    if not isinstance(num_layers, int) or num_layers < 1 or num_layers > 12:
        raise ValueError(
            f"num_layers geÃ§ersiz: {num_layers}. "
            f"1 ile 12 arasÄ±nda integer olmalÄ±"
        )
    
    # dim_feedforward validasyonu
    if not isinstance(dim_feedforward, int) or dim_feedforward < 64 or dim_feedforward > 4096:
        raise ValueError(
            f"dim_feedforward geÃ§ersiz: {dim_feedforward}. "
            f"64 ile 4096 arasÄ±nda integer olmalÄ±"
        )
    
    # dim_feedforward ve d_model iliÅŸkisi
    if dim_feedforward < d_model:
        logger.warning(
            f"   âš ï¸ dim_feedforward ({dim_feedforward}) < d_model ({d_model}). "
            f"Genellikle dim_feedforward >= d_model olmasÄ± Ã¶nerilir"
        )
    
    # dropout validasyonu
    if not isinstance(dropout, (int, float)) or dropout < 0.0 or dropout > 0.9:
        raise ValueError(
            f"dropout geÃ§ersiz: {dropout}. "
            f"0.0 ile 0.9 arasÄ±nda float olmalÄ±"
        )
    
    # Performans uyarÄ±larÄ±
    if d_model > 512:
        logger.warning(f"   âš ï¸ BÃ¼yÃ¼k d_model ({d_model}) Ã¶nemli GPU belleÄŸi gerektirebilir")
    
    if num_layers > 8:
        logger.warning(f"   âš ï¸ Ã‡ok katman ({num_layers}) eÄŸitimi yavaÅŸlatabilir")
    
    if nhead > 16:
        logger.warning(f"   âš ï¸ Ã‡ok attention head ({nhead}) hesaplama maliyeti artÄ±rabilir")
    
    logger.info(f"   âœ… Transformer konfigÃ¼rasyonu doÄŸrulandÄ±")
    
    return config


def validate_enhanced_transformer_config(config: TransformerConfig) -> TransformerConfig:
    """
    EnhancedTransformer konfigÃ¼rasyon parametrelerini doÄŸrula ve ayarla.
    
    Args:
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        DoÄŸrulanmÄ±ÅŸ konfigÃ¼rasyon
        
    Raises:
        ValueError: KonfigÃ¼rasyon geÃ§ersizse
    """
    # Orijinal sÃ¶zlÃ¼ÄŸÃ¼ bozmamak iÃ§in deep copy
    config = copy.deepcopy(config)
    
    transformer_config = config.get('transformer', {})
    
    d_model = transformer_config.get('d_model', 256)
    nhead = transformer_config.get('nhead', 12)
    num_layers = transformer_config.get('num_layers', 6)
    ff_dim = transformer_config.get('ff_dim', 512)
    dropout = config.get('model', {}).get('dropout_rate', 0.1)
    target_mode = config.get('model', {}).get('target_mode', 'binary')
    
    logger.info(f"ğŸ” EnhancedTransformer konfigÃ¼rasyonu doÄŸrulanÄ±yor...")
    logger.info(f"   d_model: {d_model}, nhead: {nhead}, layers: {num_layers}")
    
    # d_model validasyonu
    if not isinstance(d_model, int) or d_model < 64 or d_model > 2048:
        raise ValueError(
            f"d_model geÃ§ersiz: {d_model}. "
            f"64 ile 2048 arasÄ±nda integer olmalÄ±"
        )
    
    # nhead validasyonu
    if not isinstance(nhead, int) or nhead < 1 or nhead > 32:
        raise ValueError(
            f"nhead geÃ§ersiz: {nhead}. "
            f"1 ile 32 arasÄ±nda integer olmalÄ±"
        )
    
    # d_model ve nhead uyumluluÄŸu
    if d_model % nhead != 0:
        logger.warning(f"   âš ï¸ d_model ({d_model}) nhead ({nhead}) ile bÃ¶lÃ¼nemiyor")
        
        # Otomatik dÃ¼zeltme denemesi
        valid_heads = [h for h in [1, 2, 4, 8, 16, 32] if d_model % h == 0 and h <= d_model]
        if valid_heads:
            old_nhead = nhead
            nhead = max([h for h in valid_heads if h <= nhead]) or valid_heads[-1]
            transformer_config['nhead'] = nhead
            logger.warning(f"   ğŸ”§ nhead otomatik dÃ¼zeltildi: {old_nhead} â†’ {nhead}")
        else:
            raise ValueError(
                f"d_model ({d_model}) nhead ({nhead}) ile bÃ¶lÃ¼nebilir olmalÄ±. "
                f"d_model % nhead == 0 koÅŸulu saÄŸlanmalÄ±. "
                f"GeÃ§erli nhead deÄŸerleri: {[h for h in [1, 2, 4, 8, 16, 32] if d_model % h == 0]}"
            )
    
    # num_layers validasyonu
    if not isinstance(num_layers, int) or num_layers < 1 or num_layers > 16:
        raise ValueError(
            f"num_layers geÃ§ersiz: {num_layers}. "
            f"1 ile 16 arasÄ±nda integer olmalÄ±"
        )
    
    # ff_dim validasyonu
    if not isinstance(ff_dim, int) or ff_dim < 128 or ff_dim > 8192:
        raise ValueError(
            f"ff_dim geÃ§ersiz: {ff_dim}. "
            f"128 ile 8192 arasÄ±nda integer olmalÄ±"
        )
    
    # ff_dim ve d_model iliÅŸkisi
    if ff_dim < d_model * 2:
        logger.warning(
            f"   âš ï¸ ff_dim ({ff_dim}) < d_model*2 ({d_model*2}). "
            f"Genellikle ff_dim >= d_model*2 olmasÄ± Ã¶nerilir"
        )
    
    # dropout validasyonu
    if not isinstance(dropout, (int, float)) or dropout < 0.0 or dropout > 0.9:
        raise ValueError(
            f"dropout geÃ§ersiz: {dropout}. "
            f"0.0 ile 0.9 arasÄ±nda float olmalÄ±"
        )
    
    # target_mode validasyonu
    if target_mode not in ['binary', 'three_class']:
        raise ValueError(
            f"target_mode geÃ§ersiz: {target_mode}. "
            f"'binary' veya 'three_class' olmalÄ±"
        )
    
    # Performans uyarÄ±larÄ±
    if d_model > 512:
        logger.warning(f"   âš ï¸ BÃ¼yÃ¼k d_model ({d_model}) Ã¶nemli GPU belleÄŸi gerektirebilir")
    
    if num_layers > 8:
        logger.warning(f"   âš ï¸ Ã‡ok katman ({num_layers}) eÄŸitimi yavaÅŸlatabilir")
    
    if nhead > 16:
        logger.warning(f"   âš ï¸ Ã‡ok attention head ({nhead}) hesaplama maliyeti artÄ±rabilir")
    
    logger.info(f"   âœ… EnhancedTransformer konfigÃ¼rasyonu doÄŸrulandÄ±")
    
    return config


def validate_lstm_config(config: LSTMConfig) -> LSTMConfig:
    """
    LSTM konfigÃ¼rasyon parametrelerini doÄŸrula ve ayarla.
    
    Args:
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        DoÄŸrulanmÄ±ÅŸ konfigÃ¼rasyon
        
    Raises:
        ValueError: KonfigÃ¼rasyon geÃ§ersizse
    """
    # Orijinal sÃ¶zlÃ¼ÄŸÃ¼ bozmamak iÃ§in deep copy
    config = copy.deepcopy(config)
    
    model_config = config.get('model', {})
    
    hidden_size = model_config.get('hidden_size', 64)
    num_layers = model_config.get('num_layers', 2)
    dropout = model_config.get('dropout_rate', 0.45)
    
    logger.info(f"ğŸ” LSTM konfigÃ¼rasyonu doÄŸrulanÄ±yor...")
    logger.info(f"   hidden_size: {hidden_size}, layers: {num_layers}, dropout: {dropout}")
    
    # hidden_size validasyonu
    if not isinstance(hidden_size, int) or hidden_size < 16 or hidden_size > 512:
        raise ValueError(
            f"hidden_size geÃ§ersiz: {hidden_size}. "
            f"16 ile 512 arasÄ±nda integer olmalÄ±"
        )
    
    # num_layers validasyonu  
    if not isinstance(num_layers, int) or num_layers < 1 or num_layers > 4:
        raise ValueError(
            f"num_layers geÃ§ersiz: {num_layers}. "
            f"1 ile 4 arasÄ±nda integer olmalÄ±"
        )
    
    # dropout validasyonu
    if not isinstance(dropout, (int, float)) or dropout < 0.0 or dropout > 0.8:
        raise ValueError(
            f"dropout geÃ§ersiz: {dropout}. "
            f"0.0 ile 0.8 arasÄ±nda float olmalÄ±"
        )
    
    # Performans uyarÄ±larÄ±
    if hidden_size > 256:
        logger.warning(f"   âš ï¸ BÃ¼yÃ¼k hidden_size ({hidden_size}) overfitting riskini artÄ±rabilir")
    
    if num_layers > 3:
        logger.warning(f"   âš ï¸ Ã‡ok katman ({num_layers}) gradient vanishing problemine yol aÃ§abilir")
    
    logger.info(f"   âœ… LSTM konfigÃ¼rasyonu doÄŸrulandÄ±")
    
    return config


def get_model_info(model: ModelInstance) -> Dict[str, Any]:
    """
    FarklÄ± mimarilerde birleÅŸik model bilgisi al.
    
    Args:
        model: Model instance'Ä±
        
    Returns:
        Model bilgilerini iÃ§eren sÃ¶zlÃ¼k
        
    Raises:
        RuntimeError: Model bilgisi alÄ±namÄ±yorsa
    """
    try:
        if hasattr(model, 'get_model_info'):
            return model.get_model_info()
        else:
            # get_model_info metodu olmayan modeller iÃ§in fallback
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            logger.info(f"   â„¹ï¸ Model get_model_info metoduna sahip deÄŸil, temel bilgiler kullanÄ±lÄ±yor")
            
            return {
                'model_type': type(model).__name__,
                'total_parameters': total_params,
                'trainable_parameters': trainable_params,
                'architecture': 'Unknown (fallback info)'
            }
    except Exception as e:
        logger.error(f"   âŒ Model bilgisi alÄ±namadÄ±: {e}")
        raise RuntimeError(f"Model bilgisi alÄ±nÄ±rken hata: {e}") from e


def get_model_complexity_score(model: ModelInstance) -> float:
    """
    Parametre sayÄ±sÄ± ve mimariye gÃ¶re model karmaÅŸÄ±klÄ±k skoru hesapla.
    
    Args:
        model: Model instance'Ä±
        
    Returns:
        KarmaÅŸÄ±klÄ±k skoru (yÃ¼ksek = daha karmaÅŸÄ±k)
        
    Raises:
        RuntimeError: Skor hesaplanamÄ±yorsa
    """
    try:
        info = get_model_info(model)
        param_count = info['total_parameters']
        
        # Temel karmaÅŸÄ±klÄ±k parametre sayÄ±sÄ±ndan
        complexity = param_count / 100000  # Makul aralÄ±ÄŸa normalize et
        
        # Mimari-specific ayarlamalar
        if TRANSFORMER_AVAILABLE and isinstance(model, (TransformerClassifier, EnhancedTransformer)):
            # Transformer'lar attention mekanizmasÄ± nedeniyle daha karmaÅŸÄ±k
            complexity *= 1.5
            
            # Head ve katman sayÄ±sÄ±na gÃ¶re karmaÅŸÄ±klÄ±k ekle
            if hasattr(model, 'nhead'):
                complexity += model.nhead * 0.1
            if hasattr(model, 'num_layers'):
                complexity += model.num_layers * 0.2
            
            # EnhancedTransformer iÃ§in ek karmaÅŸÄ±klÄ±k
            if hasattr(model, 'target_mode') and model.target_mode == 'three_class':
                complexity *= 1.1
            
            logger.info(f"   ğŸ“Š Transformer karmaÅŸÄ±klÄ±k faktÃ¶rleri uygulandÄ±")
            
        elif ENHANCED_TRANSFORMER_V2_AVAILABLE and isinstance(model, EnhancedTransformerV2):
            # EnhancedTransformer V2 iÃ§in karmaÅŸÄ±klÄ±k
            complexity *= 1.8
            
            if hasattr(model, 'nhead'):
                complexity += model.nhead * 0.15
            if hasattr(model, 'num_layers'):
                complexity += model.num_layers * 0.25
            if hasattr(model, 'target_mode') and model.target_mode == 'three_class':
                complexity *= 1.15
                
            logger.info(f"   ğŸ“Š EnhancedTransformer V2 karmaÅŸÄ±klÄ±k faktÃ¶rleri uygulandÄ±")
            
        elif isinstance(model, PairSpecificLSTM):
            # LSTM karmaÅŸÄ±klÄ±ÄŸÄ± katman ve hidden size'a gÃ¶re
            if hasattr(model, 'num_layers'):
                complexity += model.num_layers * 0.1
            if hasattr(model, 'hidden_size'):
                complexity += model.hidden_size / 1000
            logger.info(f"   ğŸ“Š LSTM karmaÅŸÄ±klÄ±k faktÃ¶rleri uygulandÄ±")
            
        else:
            # Bilinmeyen model tipi iÃ§in default davranÄ±ÅŸ
            logger.info(f"   â„¹ï¸ Bilinmeyen model tipi ({type(model).__name__}), default skor kullanÄ±ldÄ±")
        
        final_score = round(complexity, 2)
        logger.info(f"   ğŸ“Š Model karmaÅŸÄ±klÄ±k skoru: {final_score}")
        
        return final_score
        
    except Exception as e:
        logger.error(f"   âŒ KarmaÅŸÄ±klÄ±k skoru hesaplanamadÄ±: {e}")
        raise RuntimeError(f"KarmaÅŸÄ±klÄ±k skoru hesaplama hatasÄ±: {e}") from e


def suggest_training_params(model: ModelInstance) -> Dict[str, Any]:
    """
    Model karmaÅŸÄ±klÄ±ÄŸÄ± ve tipine gÃ¶re eÄŸitim parametreleri Ã¶ner.
    
    Args:
        model: Model instance'Ä±
        
    Returns:
        Ã–nerilen eÄŸitim parametreleri
        
    Raises:
        RuntimeError: Ã–neri oluÅŸturulamÄ±yorsa
    """
    try:
        complexity = get_model_complexity_score(model)
        info = get_model_info(model)
        
        logger.info(f"ğŸ¯ EÄŸitim parametreleri Ã¶neriliyor...")
        logger.info(f"   Model karmaÅŸÄ±klÄ±ÄŸÄ±: {complexity}")
        
        if TRANSFORMER_AVAILABLE and isinstance(model, EnhancedTransformer):
            # EnhancedTransformer-specific Ã¶neriler
            
            # Model detaylarÄ±ndan ek bilgiler
            num_layers = getattr(model, 'num_layers', 6)
            nhead = getattr(model, 'nhead', 12)
            d_model = getattr(model, 'd_model', 256)
            target_mode = getattr(model, 'target_mode', 'binary')
            
            logger.info(f"   EnhancedTransformer detaylarÄ±: layers={num_layers}, heads={nhead}, d_model={d_model}")
            
            # KarmaÅŸÄ±klÄ±ÄŸa ve mimari detaylarÄ±na gÃ¶re Ã¶neri
            if complexity < 3.0 and num_layers <= 6:
                base_lr = 1e-4
                batch_size = 64
                warmup = 1000
            elif complexity < 8.0 or num_layers <= 8:
                base_lr = 5e-5
                batch_size = 32
                warmup = 2000
            else:
                base_lr = 1e-5
                batch_size = 16
                warmup = 4000
            
            # Attention head sayÄ±sÄ±na gÃ¶re LR ayarlamasÄ±
            if nhead > 16:
                base_lr *= 0.8  # Ã‡ok head varsa LR dÃ¼ÅŸÃ¼r
                logger.info(f"   ğŸ“‰ Ã‡ok attention head nedeniyle LR dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # d_model'e gÃ¶re batch size ayarlamasÄ±
            if d_model > 512:
                batch_size = max(8, batch_size // 2)
                logger.info(f"   ğŸ“¦ BÃ¼yÃ¼k d_model nedeniyle batch_size dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # Ã‡oklu sÄ±nÄ±f iÃ§in ayarlama
            if target_mode == 'three_class':
                batch_size = max(16, batch_size)
                logger.info(f"   ğŸ¯ ÃœÃ§ sÄ±nÄ±flÄ± mod iÃ§in batch_size ayarlandÄ±")
            
            suggestions = {
                'learning_rate': base_lr,
                'batch_size': batch_size,
                'warmup_steps': warmup,
                'weight_decay': 0.01 if complexity > 5.0 else 0.005,
                'scheduler': 'CosineAnnealingWarmRestarts',
                'optimizer': 'AdamW',
                'gradient_clip': 0.5,
                'model_type': 'enhanced_transformer'
            }
            
        elif ENHANCED_TRANSFORMER_V2_AVAILABLE and isinstance(model, EnhancedTransformerV2):
            # EnhancedTransformer V2-specific Ã¶neriler
            
            # Model detaylarÄ±ndan ek bilgiler
            num_layers = getattr(model, 'num_layers', 8)
            nhead = getattr(model, 'nhead', 16)
            d_model = getattr(model, 'd_model', 512)
            target_mode = getattr(model, 'target_mode', 'binary')
            
            logger.info(f"   EnhancedTransformer V2 detaylarÄ±: layers={num_layers}, heads={nhead}, d_model={d_model}")
            
            # KarmaÅŸÄ±klÄ±ÄŸa ve mimari detaylarÄ±na gÃ¶re Ã¶neri
            if complexity < 5.0 and num_layers <= 8:
                base_lr = 8e-5
                batch_size = 48
                warmup = 1500
            elif complexity < 10.0 or num_layers <= 12:
                base_lr = 3e-5
                batch_size = 24
                warmup = 3000
            else:
                base_lr = 8e-6
                batch_size = 12
                warmup = 5000
            
            # Attention head sayÄ±sÄ±na gÃ¶re LR ayarlamasÄ±
            if nhead > 24:
                base_lr *= 0.7
                logger.info(f"   ğŸ“‰ Ã‡ok attention head nedeniyle LR dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # d_model'e gÃ¶re batch size ayarlamasÄ±
            if d_model > 768:
                batch_size = max(6, batch_size // 2)
                logger.info(f"   ğŸ“¦ BÃ¼yÃ¼k d_model nedeniyle batch_size dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # Ã‡oklu sÄ±nÄ±f iÃ§in ayarlama
            if target_mode == 'three_class':
                batch_size = max(12, batch_size)
                logger.info(f"   ğŸ¯ ÃœÃ§ sÄ±nÄ±flÄ± mod iÃ§in batch_size ayarlandÄ±")
            
            suggestions = {
                'learning_rate': base_lr,
                'batch_size': batch_size,
                'warmup_steps': warmup,
                'weight_decay': 0.015 if complexity > 8.0 else 0.008,
                'scheduler': 'CosineAnnealingWarmRestarts',
                'optimizer': 'AdamW',
                'gradient_clip': 0.4,
                'model_type': 'enhanced_transformer_v2'
            }
            
        elif TRANSFORMER_AVAILABLE and isinstance(model, TransformerClassifier):
            # Transformer-specific Ã¶neriler
            
            # Model detaylarÄ±ndan ek bilgiler
            num_layers = getattr(model, 'num_layers', 4)
            nhead = getattr(model, 'nhead', 8)
            d_model = getattr(model, 'd_model', 128)
            
            logger.info(f"   Transformer detaylarÄ±: layers={num_layers}, heads={nhead}, d_model={d_model}")
            
            # KarmaÅŸÄ±klÄ±ÄŸa ve mimari detaylarÄ±na gÃ¶re Ã¶neri
            if complexity < 2.0 and num_layers <= 4:
                base_lr = 1e-3
                batch_size = 64
                warmup = 1000
            elif complexity < 5.0 or num_layers <= 6:
                base_lr = 5e-4
                batch_size = 32
                warmup = 2000
            else:
                base_lr = 1e-4
                batch_size = 16
                warmup = 4000
            
            # Attention head sayÄ±sÄ±na gÃ¶re LR ayarlamasÄ±
            if nhead > 16:
                base_lr *= 0.8  # Ã‡ok head varsa LR dÃ¼ÅŸÃ¼r
                logger.info(f"   ğŸ“‰ Ã‡ok attention head nedeniyle LR dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # d_model'e gÃ¶re batch size ayarlamasÄ±
            if d_model > 256:
                batch_size = max(8, batch_size // 2)
                logger.info(f"   ğŸ“¦ BÃ¼yÃ¼k d_model nedeniyle batch_size dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            suggestions = {
                'learning_rate': base_lr,
                'batch_size': batch_size,
                'warmup_steps': warmup,
                'weight_decay': 0.01 if complexity > 3.0 else 0.005,
                'scheduler': 'OneCycleLR',
                'optimizer': 'AdamW',
                'gradient_clip': 1.0,
                'model_type': 'transformer'
            }
            
        else:
            # LSTM-specific Ã¶neriler
            
            # Model detaylarÄ±ndan ek bilgiler
            num_layers = getattr(model, 'num_layers', 2)
            hidden_size = getattr(model, 'hidden_size', 64)
            
            logger.info(f"   LSTM detaylarÄ±: layers={num_layers}, hidden_size={hidden_size}")
            
            # KarmaÅŸÄ±klÄ±ÄŸa ve mimari detaylarÄ±na gÃ¶re Ã¶neri
            if complexity < 1.0 and num_layers <= 2:
                base_lr = 1e-3
                batch_size = 128
            elif complexity < 3.0 or num_layers <= 3:
                base_lr = 8e-4
                batch_size = 64
            else:
                base_lr = 5e-4
                batch_size = 32
            
            # Hidden size'a gÃ¶re ayarlama
            if hidden_size > 128:
                base_lr *= 0.8  # BÃ¼yÃ¼k hidden size iÃ§in LR dÃ¼ÅŸÃ¼r
                logger.info(f"   ğŸ“‰ BÃ¼yÃ¼k hidden_size nedeniyle LR dÃ¼ÅŸÃ¼rÃ¼ldÃ¼")
            
            # Katman sayÄ±sÄ±na gÃ¶re weight decay ayarlamasÄ±
            weight_decay = 0.0001 if num_layers <= 2 else 0.001
            
            suggestions = {
                'learning_rate': base_lr,
                'batch_size': batch_size,
                'weight_decay': weight_decay,
                'scheduler': 'ReduceLROnPlateau',
                'optimizer': 'AdamW',
                'gradient_clip': 0.5,
                'patience': 5,
                'model_type': 'lstm'
            }
        
        logger.info(f"   âœ… EÄŸitim parametreleri Ã¶nerildi: LR={suggestions['learning_rate']:.2e}, BS={suggestions['batch_size']}")
        
        return suggestions
        
    except Exception as e:
        logger.error(f"   âŒ Parametre Ã¶nerisi oluÅŸturulamadÄ±: {e}")
        raise RuntimeError(f"EÄŸitim parametresi Ã¶nerisi hatasÄ±: {e}") from e


def get_supported_models() -> List[str]:
    """
    Desteklenen model tiplerinin listesini dÃ¶ndÃ¼r.
    
    Returns:
        Desteklenen model tipleri listesi
    """
    return SUPPORTED_MODELS.copy()


def validate_model_compatibility(model_type: str, config: ModelConfig) -> Tuple[bool, List[str]]:
    """
    Model tipi ve konfigÃ¼rasyon uyumluluÄŸunu kontrol et.
    
    Args:
        model_type: Model tipi
        config: KonfigÃ¼rasyon sÃ¶zlÃ¼ÄŸÃ¼
        
    Returns:
        (uyumlu_mu, uyarÄ±_listesi) tuple'Ä±
    """
    warnings = []
    is_compatible = True
    
    try:
        if model_type.lower() in TRANSFORMER_ALIASES:
            validate_transformer_config(config)
        elif model_type.lower() in ENHANCED_TRANSFORMER_V2_ALIASES:
            validate_enhanced_transformer_config(config)
        elif model_type.lower() in LSTM_ALIASES:
            validate_lstm_config(config)
        else:
            warnings.append(f"Bilinmeyen model tipi: {model_type}")
            is_compatible = False
            
    except ValueError as e:
        warnings.append(str(e))
        is_compatible = False
    
    return is_compatible, warnings


__all__ = [
    'create_model',
    'get_model_info',
    'validate_transformer_config',
    'validate_enhanced_transformer_config',
    'validate_lstm_config',
    'get_model_complexity_score',
    'suggest_training_params',
    'get_supported_models',
    'validate_model_compatibility',
    'SUPPORTED_MODELS',
    'ModelConfig',
    'TransformerConfig', 
    'LSTMConfig',
    'ModelInstance'
]
